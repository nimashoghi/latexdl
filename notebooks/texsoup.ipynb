{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\\documentclass{article} % For LaTeX2e\n",
       "\\usepackage{iclr2022_conference,times}\n",
       "\n",
       "% Optional math commands from https://github.com/goodfeli/dlbook_notation.\n",
       "\\input{math_commands.tex}\n",
       "\n",
       "% font management\n",
       "\\usepackage{relsize}\n",
       "\\usepackage[T1]{fontenc}\n",
       "\\usepackage[scaled=0.90]{inconsolata}\n",
       "\n",
       "\\usepackage{hyperref}\n",
       "\\usepackage{url}\n",
       "\\usepackage{xcolor}\n",
       "\\usepackage{booktabs}\n",
       "\\usepackage{multirow}\n",
       "\\usepackage{hhline}\n",
       "% \\usepackage[table]{xcolor}\n",
       "\\usepackage{caption}\n",
       "\\usepackage{subcaption}\n",
       "\\usepackage{grffile}\n",
       "\n",
       "\\definecolor{Belize}{RGB}{41,128,185}\n",
       "\n",
       "\\newcommand{\\mr}[2]{\\multirow{#1}{*}{#2}}\n",
       "\\newcommand{\\mc}[3]{\\multicolumn{#1}{#2}{#3}}\n",
       "\n",
       "\\newcommand{\\xx}{\\mathbf{x}}\n",
       "\\newcommand{\\ff}{\\mathbf{f}}\n",
       "\\newcommand{\\zz}{\\mathbf{z}}\n",
       "\\newcommand{\\FF}{\\mathbf{F}}\n",
       "\\newcommand{\\GGG}{\\mathbf{G}}\n",
       "\\newcommand{\\GG}{\\mathbf{GNN}}\n",
       "\\newcommand{\\uu}{\\mathbf{u}}\n",
       "% \\newcommand{\\vv}{\\mathbf{v}}\n",
       "\\newcommand{\\ee}{\\mathbf{e}}\n",
       "\\newcommand{\\hh}{\\mathbf{h}}\n",
       "\\newcommand{\\ttt}{\\mathbf{t}}\n",
       "\n",
       "\\newcommand{\\mm}{\\mathbf{m}}\n",
       "\\newcommand{\\aaa}{\\mathbf{a}}\n",
       "\n",
       "\\newcommand{\\N}{\\mathcal{N}}\n",
       "\n",
       "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
       "\\newcommand{\\lz}[1]{\\textcolor{blue}{#1}}\n",
       "\\newcommand{\\ad}[1]{\\textcolor{Belize}{#1}}\n",
       "\\newcommand{\\bw}[1]{\\textcolor{purple}{#1}}\n",
       "\\newcommand{\\as}[1]{\\textcolor{brown}{#1}}\n",
       "\\newcommand{\\TODO}[1]{\\textcolor{red}{TODO: #1}}\n",
       "\n",
       "\\DeclareMathOperator{\\F}{f}\n",
       "\\DeclareMathOperator{\\Fint}{f_{int}}\n",
       "\\DeclareMathOperator{\\Fupdate}{f_{update}}\n",
       "\\DeclareMathOperator{\\TU}{TripletUpdate}\n",
       "\\DeclareMathOperator{\\EU}{EdgeUpdate}\n",
       "\\DeclareMathOperator{\\NU}{NodeUpdate}\n",
       "\\DeclareMathOperator{\\GU}{GlobalUpdate}\n",
       "\n",
       "\\DeclareMathOperator{\\TA}{TripletAggr}\n",
       "\\DeclareMathOperator{\\EA}{EdgeAggr}\n",
       "\\DeclareMathOperator{\\NA}{NodeAggr}\n",
       "\\DeclareMathOperator{\\GA}{GlobalAggr}\n",
       "\n",
       "\\DeclareMathOperator{\\AllReduce}{AllReduce}\n",
       "\n",
       "\n",
       "\\title{Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations}\n",
       "\n",
       "% Authors must not appear in the submitted version. They should be hidden\n",
       "% as long as the \\iclrfinalcopy macro remains commented out below.\n",
       "% Non-anonymous submissions will be rejected without review.\n",
       "\n",
       "\\author{Anuroop Sriram$^\\dagger$, Abhishek Das$^\\dagger$, Brandon M. Wood$^{\\ddagger \\rightarrow \\dagger}$, Siddharth Goyal$^\\dagger$, C. Lawrence Zitnick$^\\dagger$ \\\\\n",
       "$^\\dagger$Meta FAIR \\\\\n",
       "$^\\ddagger$National Energy Research Scientific Computing Center (NERSC) \\\\\n",
       "\\texttt{\\{anuroops,abhshkdz,bmwood,sidgoyal,zitnick\\}@fb.com} \\\\\n",
       "% \\And\n",
       "% Ji Q. Ren \\& Yevgeny LeNet \\\\\n",
       "% Department of Computational Neuroscience \\\\\n",
       "% University of the Witwatersrand \\\\\n",
       "% Joburg, South Africa \\\\\n",
       "% \\texttt{\\{robot,net\\}@wits.ac.za} \\\\\n",
       "% \\AND\n",
       "% Coauthor \\\\\n",
       "% Affiliation \\\\\n",
       "% Address \\\\\n",
       "% \\texttt{email}\n",
       "}\n",
       "\n",
       "% The \\author macro works with any number of authors. There are two commands\n",
       "% used to separate the names and addresses of multiple authors: \\And and \\AND.\n",
       "%\n",
       "% Using \\And between authors leaves it to \\LaTeX{} to determine where to break\n",
       "% the lines. Using \\AND forces a linebreak at that point. So, if \\LaTeX{}\n",
       "% puts 3 of 4 authors names on the first line, and the last on the second\n",
       "% line, try using \\AND instead of \\And before the third author name.\n",
       "\n",
       "\\newcommand{\\fix}{\\marginpar{FIX}}\n",
       "\\newcommand{\\new}{\\marginpar{NEW}}\n",
       "\n",
       "\\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.\n",
       "\\begin{document}\n",
       "\n",
       "\\maketitle\n",
       "\n",
       "\\begin{abstract}\n",
       "\n",
       "Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations\n",
       "has the potential to revolutionize catalyst discovery, which is a key step in\n",
       "making progress towards the energy breakthroughs needed to combat climate change.\n",
       "However, the GNNs that have proven most effective for this task are memory\n",
       "intensive as they model higher-order interactions in the graphs such as those\n",
       "between triplets or quadruplets of atoms, making it challenging to scale these models.\n",
       "In this paper, we introduce \\emph{Graph Parallelism}, a method to distribute input\n",
       "graphs across multiple GPUs, enabling us to train very large GNNs with hundreds\n",
       "of millions or billions of parameters. We empirically evaluate our method by\n",
       "scaling up the number of parameters of the recently proposed DimeNet++ %~\\citep{klicpera_dimenetpp_2020}\n",
       "and\n",
       "GemNet %~\\citep{klicpera2021gemnet} \\bw{is it common to have citations in the abstract?}\n",
       "models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset%~\\citep{OC20}\n",
       ",\n",
       "these graph-parallelized models lead to relative improvements of 1) $15\\%$ on\n",
       "the force MAE metric for the S2EF task and 2) $21\\%$ on the AFbT metric for the\n",
       "IS2RS task, establishing new state-of-the-art results.\n",
       "\n",
       "\\end{abstract}\n",
       "\n",
       "\\section{Introduction}\n",
       "\\label{sec:intro}\n",
       "\n",
       "Graph Neural Networks (GNNs)~\\citep{gori2005new,zhou2020graph} have emerged\n",
       "as the standard architecture of choice for modeling atomic systems, with\n",
       "a wide range of applications from protein structure prediction to catalyst discovery and drug design~\\citep{schutt2017quantum,gilmer2017neural,jorgensen2018neural,zitnick2020introduction,schutt2017schnet,xie2018crystal}.\n",
       "These models operate on graph-structured inputs, where nodes of the graph\n",
       "represent atoms, and edges represent bonds or atomic neighbors. Despite their\n",
       "widespread success and the availability of large molecular datasets, training massive GNNs (with up to billions of\n",
       "parameters) is an important but under-explored area. Success of\n",
       "similarly large models in computer vision, natural language processing, and speech\n",
       "recognition~\\citep{shoeybi2020megatronlm,huang2019gpipe,brown2020language,zhai2021scaling}\n",
       "suggests that scaling up GNNs could yield significant performance gains.\n",
       "\n",
       "Most previous approaches to scaling GNNs have focused on scaling small models (with up to a few million parameters) to massive graphs. These methods generally assume that we are working with a large, fixed graph, leading to the development of methods like neighborhood sampling~\\citep{jangda2021accelerating,zheng2021distdgl}\n",
       "%\\lz{or efficient partitioning that depend on the graph structure being provided during training}\n",
       "~\\citep{Jia2020ImprovingTA,Ma2019NeuGraphPD,tripathy2020reducing}.\n",
       "%\n",
       "These methods do not apply to atomic simulation datasets that contain millions of smaller graphs\n",
       "where it is necessary to consider the entire graph for prediction. Our focus is on the complementary problem of scaling to very large models for a dataset of many moderately-sized graphs ($\\sim1k$ nodes, $500k$ edges).\n",
       "\n",
       "Another limitation of existing methods is that they focus on scaling simple GNN architectures such as Graph Convolutional Networks (GCNs) that only represent lower-order interactions~\\textit{i.e.}, representations for nodes and edges, that are then updated by passing messages between neighboring nodes.\n",
       "%\n",
       "In practice, the most successful GNNs used for atomic systems also model higher-order interactions between atoms, such as the interactions between triplets or quadruplets of atoms~\\citep{klicpera_dimenetpp_2020,klicpera2021gemnet,liu2021spherical}.\n",
       "These interactions are necessary to capture the geometry of the underlying system,\n",
       "critical in making accurate predictions. Scaling such GNN architectures is challenging because even moderately-sized graphs can contain a large number of higher-order interactions. For example, a single graph with $1k$ nodes could contain several million triplets of atoms.\n",
       "%\n",
       "In this paper, we introduce \\emph{Graph Parallelism}, an approach to scale up\n",
       "such GNNs with higher-order interactions to billions of parameters,\n",
       "by splitting up the input graph across multiple GPUs.\n",
       "% In this paper, we introduce \\emph{Graph Parallelism}, an approach to split graphs across multiple GPUs, that enables us to scale such GNNs with higher-order interactions towards billions of parameters in model size.\n",
       "\n",
       "We benchmark our approach by scaling up two recent GNN architectures -- DimeNet++~\\citep{klicpera_dimenetpp_2020} and GemNet-T~\\citep{klicpera2021gemnet} -- on the Open Catalyst (OC20) dataset~\\citep{OC20}.\n",
       "%\n",
       "The OC20 dataset, aimed at discovering new catalyst materials for renewable energy storage,\n",
       "consists of $134M$ training examples spanning a wide range of adsorbates and catalyst materials.\n",
       "A GNN that can accurately predict per-atom forces and system energies on OC20\n",
       "has the potential to replace the computationally expensive quantum chemistry calculations based on Density Functional Theory (DFT) that are currently the bottleneck in computational catalysis. Our large-scale, graph-parallelized models lead to relative improvements of\n",
       "1) $15\\%$ for predicting forces on the force MAE metric (S2EF task), and 2) $21\\%$ on the AFbT metric for predicting relaxed structures (IS2RS task), establishing new state-of-the-art results on this dataset.\n",
       "\n",
       "\n",
       "% We present experimental results on the Open Catalyst 2020 (OC20) dataset~\\citep{OC20}. The OC20 dataset was developed for the task of discovering new catalyst materials. This is an important application of GNNs for atomic modeling since discovering new catalysts to drive chemical reactions is essential for addressing energy scarcity, renewable energy storage, and more broadly climate change. Traditionally, density functional theory (DFT) has been used for testing potential catalyst materials by estimating the energy of the atomic system as well as forces exerted on each atom. Unfortunately, the computational complexity of DFT limits its widespread use for testing a large number of materials. The OC20 dataset has been developed for training machine learning approximations of DFT based on GNNs. While existing ML models for approximating DFT have shown some promise, they are not accurate enough to replace DFT. We hypothesize that scaling up these models can yield large improvements in performance.\n",
       "\n",
       "% \\ad{We benchmark Graph Parallelism by scaling up two recent GNN architectures --\n",
       "% DimeNet++~\\citep{klicpera_dimenetpp_2020} and GemNet-dT~\\citep{klicpera2021gemnet} -- on\n",
       "% the Open Catalyst (OC20) dataset~\\citep{OC20}.\n",
       "% The OC20 dataset, aimed at simulating catalyst materials useful for renewable energy storage,\n",
       "% consists of $134M$ DFT relaxation trajectories spanning a wide range of adsorbate\n",
       "% and catalyst chemical compositions.\n",
       "% A GNN that can accurately predict per-atom forces and system energies on OC20\n",
       "% has the potential to significantly speed up or even replace Density Functional Theory (DFT) calculations.\n",
       "% Our large-scale, graph-parallelized models lead to relative improvements of\n",
       "% 1) $15\\%$ on the force MAE metric on the S2EF task and 2) $21\\%$ on the AFbT\n",
       "% metric on the IS2RS task, establishing new state-of-the-art results on OC20.}\n",
       "\n",
       "% In this paper, we describe the Graph Parallelism paradigm and demonstrate how to scale up two existing GNN models, DimeNet++ \\cite{} and GemNet-T \\cite{}. We empirically demonstrate the benefits of scaling up these models on the Open Catalyst 2020 (OC20) dataset aimed at simulating catalyst materials that are useful for climate change applications. These models obtain state-of-the-art results on this dataset, beating the previous SOTA by XX\\% and YY\\% respectively.\n",
       "% TODO Further claims depending on the results.\n",
       "\n",
       "% \\TODO{AD: Yet to read the approach section in detail, but we should probably add\n",
       "% a sentence or two here in the intro about energy-centric vs. force-centric force estimation as well.}\n",
       "\n",
       "\\section{Graph Parallelism}\n",
       "\\label{sec:graphparallel}\n",
       "\n",
       "\\subsection{Extended Graph Nets}\n",
       "\\label{sec:egn}\n",
       "\n",
       "\\cite{battaglia2018relational} introduced a framework called Graph Network (GN) that provides a general abstraction for many popular Graph Neural Networks (GNNs) operating on edge and node representations of graphs. We build on their work and define the Extended Graph Network (EGN) framework to include GNNs that also operate on higher order terms like triplets or quadruplets of nodes.\n",
       "\n",
       "In the Graph Network (GN) framework, a graph is defined as a $3$-tuple $\\GGG = (\\uu, V, E)$, where $\\uu$ represents global attributes about the entire graph; $V = \\{\\vv_i\\}_{i=1:N^v}$ is the set of all nodes, with $\\vv_i$ representing the attributes of node $i$; and $E = \\{(\\ee_k, r_k, s_k)\\}_{k=1:N^e}$ is the set of all edges where $\\ee_k$ represents the attributes for the edge from node $s_k$ to $r_k$. A GNN then contains a series of GN blocks\n",
       "that iteratively operate on the input graph, updating the various representations.\n",
       "\n",
       "In our \\emph{Extended Graph Network (EGN)} framework, a graph is defined as a $4$-tuple $\\GGG = (\\uu, V, E, T)$, where $\\uu, V,$ and $E$ are defined as in the Graph Network, and $T = \\{(\\ttt_m, e_{m_1}, e_{m_2}, \\ldots)\\}$ is the set of higher-order interaction terms involving edges indexed by $m_1, m_2, \\ldots$.\n",
       "\n",
       "As a concrete example, consider an atomic system represented as a graph in this framework with the nodes representing the atoms and the edges representing atomic neighbors. The node attributes $v_i$ and edge attributes $e_k$ could represent the atom's atomic numbers and distances between atoms respectively. The higher order interactions could represent triplets of atoms, \\textit{i.e.}, pairs of neighboring edges with $\\ttt_m$ representing the bond angle, which is the angle between edges that share a common node.\n",
       "Finally, the global attribute $\\uu$\n",
       "can represent the energy of the system. For clarity of exposition, we will limit our discussion to triplets in the rest of the paper, but higher order interactions can be handled in a similar manner. We denote these triplets as $(\\ttt_m, e_{m_1}, e_{m_2})$.\n",
       "\n",
       "In the EGN framework, GNNs then contain a series of \\emph{EGN blocks} that iteratively update the graph. Each EGN block consists of several \\emph{update} and \\emph{aggregation} functions that are applied to transform an input graph $(\\uu, V, E, T)$ into an output graph $(\\uu', V', E', T')$. %We call these functions $\\TU, \\TA, \\EU $ etc.\n",
       "Each EGN block starts by updating the highest order interactions, which are then aggregated before updating the next highest order interaction. For instance, the triplet representations are first updated (using $\\TU$ function) and then aggregated ($\\TA$) at each edge. These aggregated representations are then used to update the edge representation ($\\EU$). Next, the edges going into a node are aggregated and used to update the node representations, and so on. This is illustrated in figure \\ref{fig:egn_block}.\n",
       "\n",
       "% \\begin{enumerate}\n",
       "%     \\item Update each triplet representation:\n",
       "%     \\begin{equation} \\label{eq:triplet_update}\n",
       "%         \\ttt'_m = \\TU (\\ttt_m, \\ee_{m_1}, \\ee_{m_2}, \\uu)\n",
       "%     \\end{equation}\n",
       "%     \\item For each edge $k$, aggregate the updated representations of all triplets involving edge $k$:\n",
       "%     \\begin{equation} \\label{eq:triplet_aggr}\n",
       "%         \\Bar{\\ttt}'_k = \\TA (T'_k)\n",
       "%     \\end{equation}\n",
       "%     where $T'_k = \\{ (\\ttt'_m, e_{m_1}, e_{m_2}) | e_{m_1} = k \\}$.\n",
       "\n",
       "%     \\item Update each edge representation using the aggregated triplet representations:\n",
       "%     \\begin{equation} \\label{eq:edge_update}\n",
       "%         \\ee'_k = \\EU (\\ee_k, \\vv_{r_k}, \\vv_{s_k}, \\Bar{\\ttt}'_k, \\uu)\n",
       "%     \\end{equation}\n",
       "%     \\item For each node $i$, aggregate the updated edge representations for all edges incident on node $i$:\n",
       "%     \\begin{equation} \\label{eq:edge_aggr}\n",
       "%         \\Bar{\\ee}'_i = \\EA (E'_i)\n",
       "%     \\end{equation}\n",
       "%     where $E'_i = \\{ (\\ee'_k, r_k, s_k) | r_k = i \\}$.\n",
       "\n",
       "%     \\item Update each node representation using the aggregated edge representations:\n",
       "%     \\begin{equation} \\label{eq:node_update}\n",
       "%         \\vv'_i = \\NU (\\vv_i, \\Bar{\\ee}'_i, \\uu)\n",
       "%     \\end{equation}\n",
       "\n",
       "%     \\item Finally, aggregate information from all the nodes and update the global attributes:\n",
       "%     \\begin{equation} \\label{eq:node_update2}\n",
       "%       \\Bar{\\vv}' = \\NA (V')\n",
       "%     \\end{equation}\n",
       "%     \\begin{equation} \\label{eq:node_aggr}\n",
       "%       \\uu' = \\GU (\\Bar{\\vv}', \\uu)\n",
       "%     \\end{equation}\n",
       "%     where $V' = \\{\\vv'_i\\}_{i=1:N^v}$\n",
       "\n",
       "% \\end{enumerate}\n",
       "\n",
       "Many GNNs can be cast in the EGN framework using appropriate update and aggregation functions.\n",
       "\n",
       "% \\begin{figure}\n",
       "%     \\centering\n",
       "%     \\includegraphics[width=0.8\\linewidth]{iclr2022/extended_graph_nets.pdf}\n",
       "%     \\caption{Forward computation of an EGN block. First, each triplet representation is updated using the $\\TU$ function, followed by an aggregation of these updated representations ($\\TA$). These aggregated values are then used to update the edge representations ($\\EU$), followed by an edge aggregation ($EA$). This process is continued in a similar manner to update the node and global representations as well.}\n",
       "%     \\label{fig:egn_block}\n",
       "% \\end{figure}\n",
       "\n",
       "% \\begin{figure}\n",
       "%     \\centering\n",
       "%     \\includegraphics[width=0.8\\linewidth]{iclr2022/graph_parallel.pdf}\n",
       "%     \\caption{Distributed computation of an EGN block using graph parallelism. The graph is split up among the different GPUs such that processing unit $p$ contains its subset of triplets $T^{(p)}$ in memory, along with the entire set of edges $E$ and nodes $V$. The triplet attributes are updated first, followed by a triplet aggregation to update the edge attributes locally. Next, an allreduce operation is performed to update all edge attributes globally. We repeat this process to update the node and global attributes.}\n",
       "%     \\label{fig:graph_parallel}\n",
       "% \\end{figure}\n",
       "\n",
       "\\begin{figure}[h]\n",
       "\\centering\n",
       "    \\begin{subfigure}[b]{0.9\\textwidth}\n",
       "        \\centering\n",
       "        \\includegraphics[width=\\textwidth]{extended_graph_nets.pdf}\n",
       "        \\caption{Forward computation of an EGN block. First, each triplet representation is updated ($\\TU$ function),\n",
       "        followed by aggregation of these updated representations ($\\TA$).\n",
       "        Next, these aggregated values are used to update edge representations ($\\EU$), followed by edge aggregation ($\\EA$),\n",
       "        and finally node and global updates.}\n",
       "        \\label{fig:egn_block}\n",
       "    \\end{subfigure}\n",
       "\n",
       "    \\vspace{0.2cm}\n",
       "    \\begin{subfigure}[b]{0.9\\textwidth}\n",
       "    \\centering\n",
       "    \\includegraphics[width=\\textwidth]{graph_parallel.pdf}\n",
       "    \\caption{Distributed computation of an EGN block. The graph is split up among the different GPUs such that processing unit $p$ contains its subset of triplets $T^{(p)}$ in memory, along with all edges $E$ and nodes $V$. The triplet attributes are updated in parallel, followed by a triplet aggregation to locally update the edge attributes. Next, an {\\tt allreduce} operation is performed to update all edge attributes globally. We continue this process to update the node and global attributes.}\n",
       "    \\label{fig:graph_parallel}\n",
       "\n",
       "    \\end{subfigure}\n",
       "\n",
       "    \\label{Sequential and distributed computation of an EGN block.}\n",
       "\\end{figure}\n",
       "\n",
       "\\subsection{Graph Parallelism for Extended Graph Nets}\n",
       "\\label{sec:gp-egn}\n",
       "\n",
       "Training large EGNs can be challenging even on moderately sized graphs because of the large memory footprint required in storing and updating the representation for each triplet, edge, and node. In many applications, the number of edges is one or two orders of magnitude larger than the number of nodes, while the number of triplets is one or two orders of magnitude larger than the number of edges. Thus, storing and updating the triplet representations is often the bottleneck in terms of GPU memory and compute. Many recent methods such as \\citep{klicpera_dimenetpp_2020,klicpera2021gemnet} overcome this problem by using a very low-dimensional representation for the triplets. However, we found this to significantly reduce model capacity leading to underfitting for some applications with large training datasets. It is necessary to overcome these memory limitations for better generalization.\n",
       "\n",
       "One way to avoid the memory limits is to distribute the computation across multiple GPUs. For a graph neural network, a natural choice to distribute the computation is by splitting the graph.\n",
       "The update functions in an EGN are easy to apply in parallel since they are applied independently for each triplet, edge, or node. It is substantially more challenging to apply the aggregations in parallel. To simplify parallel aggregation, we restrict ourselves to aggregation functions that are commutative and associative. This is not limiting in practice since most popular GNN architectures use sum or mean aggregations which can be implemented in this manner.\n",
       "\n",
       "We will now describe the implementation of the distributed EGN block. Suppose we have access to $P$ processing units that we wish to split the graph computation over. Each unit $p$ is responsible for computing the updates to a subset of triplets, edges and nodes, that we denote by $T^{(p)}, E^{(p)},$ and $V^{(p)}$, respectively. At the beginning of computation, we split the graph so that the processing unit $p$ contains its subset of triplets $T^{(p)}$ in memory, along with the entire set of edges $E$ and nodes $V$. During the forward pass, we first update each set of triplets $T^{(p)}$ in parallel to obtain $T'^{(p)}$, followed by a local triplet aggregation. Next, an all-reduce operation is performed on these locally aggregated triplet representations to obtain globally aggregated triplet representations. These globally aggregated representations are then used to update edges in parallel, and the process is repeated for nodes and ultimately for the global graph level representation.\n",
       "% \\bw{the second half of previous sentence is a bit confusing. If we want to follow the EGN framework should say something like the following} \\bw{During the forward pass, we first update each set of triplets $T^{(p)}$ in parallel, followed by local triplet aggregation. Next, an all-reduce operation is performed to aggregate these locally updated triplet representations into globally updated triplet representations, $T'$. The updated triplet representations are then used to update edges ($E'$) and the process is repeated for nodes ($V'$) and ultimately for the global graph level update ($\\uu'$). This process is illustrated ...}  Next, an all-reduce operation is performed to aggregate these locally updated edge representations into globally updated edge representations, $E'$. Next, we repeat the same process with edges to update the node representations, and with the node representations to update the global features.\n",
       "Figure~\\ref{fig:graph_parallel} shows this.\n",
       "\n",
       "% Computation then proceeds as follows:\n",
       "%\\lz{Okay it makes sense know why you wrote out all the steps above. It might be better to put both of these side by side in a figure? I.e., one for serial graph computation and one for parallel. This will also save space probably.}\n",
       "\n",
       "% \\begin{enumerate}\n",
       "%     \\item In each processor $p$ in parallel, update the representations for each triplet in $T^{(p)}$ in parallel using equation \\ref{eq:triplet_update}.\n",
       "%     \\item For each edge $k$, in each processor $p$, locally aggregate the triplet representations, followed by an all-reduce for global aggregation.\n",
       "%     \\begin{equation} \\label{eq:triplet_aggr2}\n",
       "%     \\begin{split}\n",
       "%         \\Bar{\\ttt}'^{(p)}_k &= \\TA (T'^{(p)}_k) \\\\\n",
       "%         \\Bar{\\ttt}'_k &= \\AllReduce ( \\{ \\Bar{\\ttt}'^{(p)}_k \\}_{p=1:P} )\n",
       "%     \\end{split}\n",
       "%     \\end{equation}\n",
       "%     At this stage, all processors have the aggregated representations $\\Bar{\\ttt}'_k$.\n",
       "\n",
       "%     \\item In each processor $p$ in parallel, update the representations for each edge in $E^{(p)}$ in parallel using equation \\ref{eq:edge_update}.\n",
       "%     \\item For each node $i$, in each processor $p$, locally aggregate the edge representations, and then perform an all-reduce step to globally aggregate the edge representations.\n",
       "\n",
       "%     \\begin{equation} \\label{eq:edge_aggr2}\n",
       "%     \\begin{split}\n",
       "%         \\Bar{\\ee}'^{(p)}_i &= \\EA (E'^{(p)}_i) \\\\\n",
       "%         \\Bar{\\ee}'_i &= \\AllReduce ( \\{ \\Bar{\\ee}'^{(p)}_i \\}_{p=1:P} )\n",
       "%     \\end{split}\n",
       "%     \\end{equation}\n",
       "\n",
       "%     \\item In each processor $p$ in parallel, update the representations for each node in $V^{(p)}$ using equation \\ref{eq:node_update}.\n",
       "\n",
       "%     \\item Finally, locally aggregate node representations, all-reduce the results and update the global attributes.\n",
       "%     \\begin{equation} \\label{eq:node_update3}\n",
       "%     \\begin{split}\n",
       "%       \\Bar{\\vv}'^{(p)} &= \\NA (V'^{(p)}) \\\\\n",
       "%       \\Bar{\\vv}' &= \\AllReduce ( \\{ \\Bar{\\vv}'^{(p)}_i \\}_{p=1:P} ) \\\\\n",
       "%       \\uu' &= \\GU (\\Bar{\\vv}', \\uu)\n",
       "%     \\end{split}\n",
       "%     \\end{equation}\n",
       "%     where $V' = \\{\\vv'_i\\}_{i=1:N^v}$.\n",
       "\n",
       "% \\end{enumerate}\n",
       "\n",
       "In this framework, the highest order interaction attributes are never communicated across processors. Therefore, the communication time is bound by the number of lower order node interactions. In our example, the triplet representations are not communicated, while the edge and node representations are communicated once per EGN block, making the total communication cost equal to $O(N_v D_v + N_e D_e)$, where $D_v$ and $D_e$ are the dimensions of node and edge representations. This makes it possible to work with a large number of triplets and large triplet representations. In section \\ref{sec:gp_atoms}, we show concrete instantiations of this framework for two contemporary GNNs.\n",
       "\n",
       "% triplet representations are never communicated across processors. This makes it possible to work with large triplet representations without running out of memory. The edge representations and node representations are communicated once per EGN block, making the total communication cost equal to $O(N_v N_e D_v D_e)$ where $D_v$ and $D_e$ are the dimensions of node and edge representations.\n",
       "\n",
       "% \\lz{If you did use this for interactions higher order than triplets, it might not work since you have to enumerate at triplets per GPU correct? If this isn't the case, I'd call it out.}\n",
       "% \\as{Actually, I have second thoughts about adding this. Given our definition of EGN, we only have one set of higher order interactions that need to be updated before updating edges. With this definition, the approach works (and works for GemNet-Q as well). If we had a model that uses the quadruplets to update the triplets, then triplets to edges, and so on, that would change the definition of the EGN to a 6-tuple (u, V, E, T, Q).}\n",
       "% Since this approach only communicates node and edge representation, and avoids communicating triplet representations, the size of the graphs that we can apply this method to is limited by the number of edges. If we apply this to an EGN with even higher order interactions, such as quadruplets of atoms, that need to be updated before triplets are updated, then the size of the graphs will be limited by the number of triplets\n",
       "\n",
       "\\section{Graph Parallelism for Atomic Simulations} \\label{sec:gp_atoms}\n",
       "\n",
       "In this section, we present two concrete examples of using GNNs for the problem of predicting the energy and the forces for an atomic system, modeled as a graph whose nodes represent the atoms and whose edges represent the atoms' neighbors. The GNN takes such a graph as input and predicts the energy of the entire system as well as a 3D force vector on each atom. Two paradigms have been proposed in the literature, that we call \\emph{energy-centric} and \\emph{force-centric} approaches. These approaches differ in how they estimate forces and whether they are energy conserving, which is an important physical property. We begin by describing the components shared by both approaches, followed by one recent model in each paradigm.\n",
       "\n",
       "\\subsection{Inputs and Outputs}\n",
       "\n",
       "The inputs to the network are 3D positions $\\xx_i \\in \\R^3$ and atomic number $z_i$ for each atom $i \\in \\{1\\ldots n\\}$.\n",
       "The outputs are the per-atom forces $\\ff_i \\in \\R^3$ and the energy $E \\in \\R$ of the entire structure. The distance between atoms $i$ and $j$ is denoted as $d_{ij} = || \\xx_i - \\xx_j ||$. If edges $(i, j)$ and $(j, k)$ exist, then $(i, j, k)$ defines a \\emph{triplet} in the graph and we denote the angle between the edges as $\\alpha_{kj,ji}$.\n",
       "\n",
       "The input graph is constructed with each atom $t$ (\\emph{target}) as a node and the edges representing the atom's neighbors $s \\in N_t$ where $N_t$ contains all atoms $s$ (\\emph{source}) within a distance $\\delta$, which is treated as a hyperparameter. Each edge has a corresponding message $m_{st}$ that passes information from source atom $s$ to target atom $t$.\n",
       "\n",
       "\\subsection{Estimating Forces and Energy}\n",
       "\n",
       "As previously stated, there are two paradigms for estimating the energy and forces for an atomic system: \\emph{energy-centric} and \\emph{force-centric}. In energy-centric models, the model first computes the energy $E$ by applying a forward pass of the GNN: $E = \\GG(\\xx, \\zz)$, where $\\xx$, and $\\zz$ represent the atomic positions and atomic numbers respectively. The forces are then computed as the negative gradient of the energy with respect to atomic positions by using backpropagation: $\\ff = -\\nabla_{\\xx} E$.\n",
       "\n",
       "In force-centric models, the energy and forces are both computed directly during the forward propagation: $E, \\FF = \\GG(\\xx, \\zz)$ where $\\FF$ represents the matrix of all atomic forces. Force-centric models tend to be more efficient in terms of computation time and memory usage compared to the energy-centric models. However, energy-centric models guarantee that the forces are energy conserving which is an important physical property satisfied by atomic systems. In this work, we demonstrate the benefits of scaling up GNNs in both paradigms.\n",
       "\n",
       "\\subsection{Energy-Centric model: DimeNet++}\n",
       "\n",
       "DimeNet++~\\citep{klicpera_dimenetpp_2020} is a recently proposed energy-centric model for atomic systems. In this model, the edges are represented by a feature representation $\\mm_{ji}$, which are iteratively updated using both directional information (via bond angles) as well as the interatomic distances. The edges are initially represented using a radial basis function (RBF) representation $\\ee^{(ji)}_{RBF}$ of their lengths $d_{ji}$. The triplets are represented using a spherical basis function (SBF) expansion $\\aaa^{(kj,ji)}_{SBF}$ of the distances $d_{kj}$ as well as bond angles $\\alpha_{(kj,ji)}$. In each block, the messages are updated as:\n",
       "%\n",
       "\\begin{equation}\n",
       "    \\mm_{ji}' = \\Fupdate (\\mm_{ji}, \\sum_{k\\in \\N_j \\textbackslash \\{i\\}} \\Fint(\\mm_{kj}, \\ee^{(ji)}_{RBF}, \\aaa^{(kj,ji)}_{SBF}))\n",
       "\\end{equation}\n",
       "%\n",
       "where the interaction function $\\F_{int}$ corresponds to the $\\TU$ function, the summation corresponds to the $\\TA$ function, and the update function $\\F_{update}$ to the $\\EU$ function respectively. The interaction function consists of a Hadamard product of the embeddings, followed by a multi-layer perceptron. To speed up these computations, the embeddings are projected down to a smaller dimension before computing interactions, and later projected back up.\n",
       "\n",
       "The updated messages are then fed as input to an output block that sums them up per atom $i$ to obtain a per-atom representation: $\\hh_i = \\sum_j \\mm'_{ji}$ ($\\EA$ function). These are then transformed by another MLP to obtain the node representation $\\vv'_i$ ($\\NU$ function).\n",
       "\n",
       "Thus, DimeNet++ is a case of EGN, and we closely follow the recipe from Sec.~\\ref{sec:gp-egn} to parallelize it.\n",
       "\n",
       "\\subsection{Force-Centric model: GemNet}\n",
       "\n",
       "GemNet~\\citep{klicpera2021gemnet} extends DimeNet++ in a number of ways, including the addition of quadruplets as well as a force-centric version. Here, we focus only on the force-centric GemNet-T model since it was recently shown to obtain state-of-the-art results on the OC20 dataset\\footnote{{\\href{https://opencatalystproject.org/leaderboard.html}{\\tt opencatalystproject.org/leaderboard.html}}}. GemNet-T largely follows the structure of the DimeNet++ model, but includes some modifications.\n",
       "\n",
       "First, GemNet-T uses a bilinear layer instead of the Hadamard product for the interaction function. This is made efficient by optimally choosing the order of operations in the bilinear function to minimize computation. Second, GemNet-T maintains an explicit embedding for each atom that is first updated by aggregating the directional embeddings involving that atom, similar to DimeNet++. Next, the updated atom embedding is used to update each of the edge embeddings. This creates a second edge update function, $\\EU'$, that is run after the node embeddings are updated. Third, GemNet makes use of symmetric message passing, that is the messages $\\mm_{ji}$ and $\\mm_{ij}$ that are on the same edge, but in different directions, are coupled.\n",
       "In a parallel implementation, this step requires an additional all-reduce step since messages $\\mm_{ji}$ and $\\mm_{ij}$ could be on different processors.\n",
       "\n",
       "Thus, GemNet-T largely follows the EGN framework, with a few minor deviations from the standard formulation. The distributed EGN implementation described in section \\ref{sec:gp-egn} can be used for the GemNet-T as well, but with additional communication steps to account for the second edge update function and symmetric message passing.\n",
       "\n",
       "\\section{Experiments}\n",
       "% setup -- OCP dataset, compute infra, experiment parameters\n",
       "\n",
       "In this section, we present the results of our scaling experiments on the Open Catalyst 2020 (OC20) dataset~\\citep{OC20}. The OC20 dataset contains over 130 million atomic structures used to train models for predicting forces and energies during structure relaxations. We report results for three tasks: 1) Structure to Energy and Forces (S2EF) that involves predicting energy and forces for a given structure; 2) Initial Structure to Relaxed Energy (IS2RE) that involves predicting the relaxed energy for a given initial structure; and 3) Initial Structure to Relaxed Structure (IS2RS) which involves performing a structure relaxation using the predicted forces. DimeNet++ and GemNet-T are the current state-of-the-art energy-centric and force-centric models respectively.\n",
       "\n",
       "\\subsection{Experimental Setup}\n",
       "\n",
       "\\textbf{DimeNet++-XL}. Our DimeNet++ model consists of $B=4$ interaction blocks, with a hidden dimension of $H = 2048$, an output block dimension of $D = 1536$, and intermediate triplet dimension of $T = 256$. This model has about $240M$ parameters, which is over $20\\times$ larger than the DimeNet++-large model used in~\\citep{OC20}. We call this model \\emph{DimeNet++-XL}. The model was trained with the AdamW optimizer~\\citep{kingma2014adam,loshchilov2019decoupled} starting with an initial learning rate of $10^{-4}$, that was multiplied by $0.8$ whenever the validation error plateaus. The model was trained with an effective batch size of 128 on 256 Volta 32GB GPUs with a combination of data parallel and graph parallel training: each graph was split over 4 GPUs with data parallel training across groups of 4 GPUs.\n",
       "\n",
       "\\textbf{GemNet-XL}. Our GemNet model consists of $B = 6$ interaction blocks, with an edge embedding size of $E=1536$, triplet embedding size of $T=384$ and embedding dimension of the bilinear layer of $B=192$. We found that it was beneficial to use a small atom embedding size of $A=128$, much smaller than the previous SOTA GemNet model\\footnote{\\href{https://discuss.opencatalystproject.org/t/new-gemnet-dt-code-results-model-weights/102}{\\tt discuss.opencatalystproject.org/t/new-gemnet-dt-code-results-model-weights/102}}. This model has roughly 300M parameters, which is about $10\\times$ larger than the previous SOTA model. However, since we reduced the atom embedding dimension and increased the edge and triplet dimensions, the total amount of compute and memory usage is significantly larger. We call this model \\emph{GemNet-XL}. We followed the same training procedure as with DimeNet++, except for a starting learning rate of $2\\times10^{-4}$.\n",
       "\n",
       "% TODO: Describe Loss function, cutoff radius etc.\n",
       "\n",
       "\\subsection{Structure to Energy and Forces (S2EF)}\n",
       "The Structure to Energy and Forces task takes an atomic structure as input and predicts the energy of the entire structure and per-atom forces. The S2EF task has four metrics: the energy and force Mean Absolute Error (MAE), the Force Cosine Similarity, and the Energy and Forces within a Threshold (EFwT). EFwT indicates the percentage of energy and force predictions below a preset threshold.\n",
       "\n",
       "Table \\ref{tab:s2ef_results} compares the top models on the Open Catalyst Project leaderboard$^1$ with the GemNet-XL model. GemNet-XL obtains a roughly $16\\%$ lower force MAE and an $8\\%$ lower energy MAE relative to the previous state-of-the-art. Further, GemNet-XL improves the EFwT metric more than $50\\%$ relative to the previous best, although the value is still very small. These results indicate that model scaling is beneficial for the S2EF task.\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{5pt}\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{lrrrrrr}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{\\#Params}} & \\mr{2}{\\textbf{Training GPU-days}} & \\mc{4}{c}{\\textbf{S2EF Test}}\\\\\n",
       "        & & & Energy MAE (eV)$\\downarrow$ & Force MAE (eV/A)$\\downarrow$ & Force Cos$\\uparrow$ & EFwT$\\uparrow$\\\\\n",
       "        \\midrule\n",
       "        ForceNet-Large~\\citep{hu2021forcenet} & 34.8M & 194 & 2.2628\t &0.03115\t& 0.5195\t& 0.01\\% \\\\\n",
       "        DimeNet++-Large~\\citep{klicpera_dimenetpp_2020} & 10.8M & 1600 & 31.5409\t& 0.03132\t& 0.5440\t& 0.00\\% \\\\\n",
       "        SpinConv~\\citep{shuaibi_rotation_2021} & 8.9M & 76 & 0.3363 &\t0.02966 &\t0.5391 &\t0.45\\%\t\\\\\n",
       "        GemNet-T~\\citep{klicpera2021gemnet} & 31M & 47 & 0.2924 & 0.02422 &\t0.6162\t& 1.20\\% \\\\\n",
       "        \\midrule\n",
       "        % DimeNet++-XL (force) & 240M & XX & - & XX & XX & XX \\\\\n",
       "        GemNet-XL & 300M & 1962 & \\textbf{0.2701} & \\textbf{0.02040} & \\textbf{0.6603} & \\textbf{1.81\\%} \\\\\n",
       "        \\bottomrule\n",
       "         % TODO: Add #params.\n",
       "    \\end{tabular}}\n",
       "    \\caption{Experimental results on the S2EF task comparing our GemNet-XL to the top entries on the Open Catalyst leaderboard, showing metrics averaged across the 4 test datasets.}\n",
       "    \\label{tab:s2ef_results}\n",
       "    \\vspace{-5pt}\n",
       "\\end{table*}\n",
       "\n",
       "\\subsection{Initial Structure to Relaxed Structure (IS2RS)}\n",
       "The Initial Structure to Relaxed Structure (IS2RS) task involves taking an initial atomic structure and predicting the atomic structure that minimizes the energy. This is performed by iteratively predicting the forces on each atom and then using these forces to update the atomic positions. This process is repeated until convergence or 200 iterations. There are three metrics for this task: Average Distance within Threshold (ADwT), that measures the fraction of final atomic positions within a distance threshold of the ground truth; Forces below Threshold (FbT), which measures whether a true energy minimum was found (\\textit{i.e.}, forces are smaller than a preset threshold); and, the Average Forces below Threshold (AFbt), which averages the FbT over several thresholds.\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.8\\linewidth}{!}{\n",
       "    \\small\n",
       "    \\begin{tabular}{lrcrrr}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{\\#Params}} & \\textbf{Training} & \\mc{3}{c}{\\textbf{IS2RS Test}} \\\\\n",
       "        & & \\textbf{Dataset} & AFbT$\\uparrow$ & ADwT$\\uparrow$ & FbT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        SpinConv~\\citep{shuaibi_rotation_2021} & 8.9M & S2EF-All & 16.67\\% &\t53.62\\% &\t0.05\\% \\\\\n",
       "        DimeNet++~\\citep{klicpera_dimenetpp_2020} & 1.8M & S2EF 20M + MD\t& 17.15\\%\t& 47.72\\%\t& 0.15\\% \\\\\n",
       "        DimeNet++-large~\\citep{klicpera_dimenetpp_2020} & 10.8M & S2EF-All & 21.82\\% & 51.68\\% & 0.40\\% \\\\\n",
       "        GemNet-T~\\citep{klicpera2021gemnet} & 31M & S2EF-All &\t27.60\\%\t& 58.68\\%\t& 0.70\\% \\\\\n",
       "        \\midrule\n",
       "        DimeNet++-XL & 240M & S2EF 20M + MD & \\textbf{33.44\\%} & 59.21\\% & \\textbf{1.25\\%} \\\\\n",
       "        GemNet-XL & 300M & S2EF-All & 30.82\\% & \\textbf{62.65\\%} & 0.90\\%\\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Results on the IS2RS task comparing our models to the top entries on the Open Catalyst leaderboard, showing metrics averaged across the 4 test datasets. The DimeNet++ and DimeNet++-XL models were trained on the S2EF 20M + MD dataset, that contains additional molecular dynamics data and has been shown to be helpful for the IS2RS task~\\citep{OC20}.}\n",
       "    \\label{tab:is2rs_results}\n",
       "    \\vspace{-15pt}\n",
       "\\end{table*}\n",
       "\n",
       "\n",
       "\\begin{table*}[h]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.6\\linewidth}{!}{\n",
       "    \\small\n",
       "    \\begin{tabular}{llrr}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{Approach}} & \\mc{2}{c}{\\textbf{IS2RE Test}}\\\\\n",
       "        & & Energy MAE (EV)$\\downarrow$ & EwT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        SpinConv~\\citep{shuaibi_rotation_2021} & Relaxation & 0.4343\t& 7.90\\% \\\\\n",
       "        GemNet-T~\\citep{klicpera2021gemnet} & Relaxation & 0.3997\t& 9.86\\% \\\\\n",
       "        Noisy Nodes~\\citep{godwin_iclr22} & Direct\t& 0.4728\t& 6.50\\%\t\\\\\n",
       "        3D-Graphormer~\\citep{graphormer} & Direct\t& 0.4722\t& 6.10\\%\t\\\\\n",
       "        \\midrule\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3712} & \\textbf{11.13\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4623 & 5.60\\% \\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Results on the IS2RE task comparing our GemNet-XL to the top entries on the Open Catalyst leaderboard, showing metrics averaged across the 4 test datasets.}\n",
       "    \\label{tab:is2re_results}\n",
       "\\end{table*}\n",
       "\n",
       "Table \\ref{tab:is2rs_results} shows the results on the IS2RS task, comparing our models with the top few models on the Open Catalyst leaderboard. Both the DimeNet++-XL and GemNet-XL models outperform all existing models. The DimeNet++-XL model obtains a relative improvement of 53\\%  on the AFbT metric, and more than triples the FbT metric compared to the DimeNet-large model. The GemNet-XL model obtains similar improvements compared to the smaller GemNet-T model. These results underscore the importance of model scaling for this task.\n",
       "\n",
       "\\subsection{Initial Structure to Relaxed Energy (IS2RE)}\n",
       "\n",
       "The Initial Structure to Relaxed Energy (IS2RE) task takes an initial atomic structure and attempts to predict the energy of the structure after it has been relaxed. Two approaches can be taken to address this problem, the direct and the relaxation approaches~\\citep{OC20}. In the direct approach, we treat this task as a regression problem, and train a model to directly estimate the relaxed energy for a given atomic structure. The relaxation approach first estimates the relaxed structure (IS2RS task) after which the energy is estimated using the relaxed structure as input. Relaxation approaches typically outperform the direct approaches, though they are generally two orders of magnitude slower during inference time due to the need to estimate the relaxed structure.\n",
       "\n",
       "Table \\ref{tab:is2re_results} compares our GemNet-XL model to the top three models from the Open Catalyst Project leaderboard. There are two metrics for the IS2RE task: energy Mean Absolute Error (MAE) and the Energy within Threshold (EwT) which measures the percentage of time the predicted energy is within a threshold of the true energy. Table \\ref{tab:is2re_results} shows that the GemNet-XL model obtains a roughly 8\\% lower energy MAE and a 12\\% higher EwT compared to the previous best, which is the smaller GemNet-T model, demonstrating the benefits of scaling up IS2RE models.\n",
       "\n",
       "Since direct IS2RE models are very fast at inference time, there are applications where they are more useful than relaxation based approaches.\n",
       "It is possible to convert a trained S2EF model into a direct IS2RE model by fine-tuning it on the IS2RE training data. We finetune our GemNet-XL model in this manner for 5 epochs, starting with an initial learning rate of $3\\times10^{-5}$ that is exponentially decayed by multiplying with $0.95$ at the end of each epoch. The resuting model -- GemNet-XL-FT -- obtains a relative improvement of ${\\sim}2\\%$ on energy MAE compared to\n",
       "3D-Graphormer~\\citep{graphormer}, the current state-of-the-art direct approach (Table \\ref{tab:is2re_results}).\n",
       "\n",
       "% \\begin{table*}[t]\n",
       "%     \\centering\n",
       "%     \\renewcommand{\\arraystretch}{1.0}\n",
       "%     \\setlength{\\tabcolsep}{6pt}\n",
       "%     % \\resizebox{0.97\\linewidth}{!}{\n",
       "%     \\begin{tabular}{c|c|c|c|c|c}\n",
       "%         \\toprule\n",
       "%         \\textbf{Edge Dim} & \\textbf{Atom Dim} & \\textbf{Triplet Dim} & \\textbf{Bilinear Dim} & \\textbf{Num Params} & \\textbf{Num GPUs}\\\\\n",
       "%         \\midrule\n",
       "%         1024 & 1024 & 128 & 64 & 125.4M & 1 \\\\\n",
       "%         1504 & 1504 & 160 & 96 & 269.9M & 2 \\\\\n",
       "%         2048 & 2048 & 256 & 128 & 500.5M & 4 \\\\\n",
       "%         3072 & 3072 & 384 & 192 & 1.12B & 8 \\\\\n",
       "%         \\bottomrule\n",
       "%     \\end{tabular}\n",
       "%     % }\n",
       "%     \\caption{Model hyperparameters for the scaling analysis. Number of interaction blocks is kept fixed at 3.\n",
       "%     For weak scaling efficiency by batch size, we only use the 125.4M parameter model, and for\n",
       "%     weak scaling efficiency by model size, we train all 4 model sizes on different number of GPUs.}\n",
       "%     \\label{tab:scaling_models}\n",
       "%     \\vspace{-14pt}\n",
       "% \\end{table*}\n",
       "\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{c|c|c|c|c|c|c|c}\n",
       "        \\toprule\n",
       "        \\textbf{\\#Blocks} & \\textbf{Node Dim} & \\textbf{Edge Dim} & \\textbf{Trip Dim} & \\textbf{Bil Dim} & \\textbf{Params} & \\textbf{\\#GP GPUs} & \\textbf{\\#GP+DP GPUs}\\\\\n",
       "        \\midrule\n",
       "        3 & 1280 & 768 & 128 & 64 & 125M & 1 & 32 \\\\\n",
       "        4 & 1536 & 1024 & 192 & 96 & 245M & 2 & 64 \\\\\n",
       "        6 & 1792 & 1184 & 288 & 160 & 480M & 4 & 128 \\\\\n",
       "        8 & 2320 & 1302 & 512 & 288 & 960M & 8 & 256 \\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    }\n",
       "    \\caption{Model hyperparameters for the scaling analysis. ``\\#GP GPUs'' denotes the number of GPUs over which the graph is distributed over for pure graph parallel training on a single node. ``\\#GP+DP GPUs'' denotes the total number of GPUs used to train with graph parallel training together with 32-way data parallel training.}\n",
       "    \\label{tab:scaling_models}\n",
       "    \\vspace{-14pt}\n",
       "\\end{table*}\n",
       "\n",
       "\n",
       "% \\begin{figure}[h]\n",
       "%      \\begin{subfigure}[b]{0.30\\textwidth}\n",
       "%          \\centering\n",
       "%          \\includegraphics[scale=0.2]{Weak Scaling (batch size).pdf}\n",
       "%          \\caption{Weak scaling efficiency by batch size. The model size is fixed and batch size is set proportional to the number of GPUs.}\n",
       "%          \\label{fig:scaling_batch}\n",
       "%      \\end{subfigure}\n",
       "%      \\quad\n",
       "%      \\begin{subfigure}[b]{0.34\\textwidth}\n",
       "%          \\centering\n",
       "%          \\includegraphics[scale=0.22]{Weak Scaling (model size).pdf}\n",
       "%          \\caption{Weak scaling efficiency by model size. The batch size is fixed and model computational cost is scaled proportional to the number of GPUs.}\n",
       "%          \\label{fig:scaling_model}\n",
       "%      \\end{subfigure}\n",
       "%      \\quad\n",
       "%      \\begin{subfigure}[b]{0.29\\textwidth}\n",
       "%          \\centering\n",
       "%          \\includegraphics[scale=0.2]{gp_ddp_v1.png}\n",
       "%          \\caption{Graph (blue) and graph + data (red) parallel as a function of number of GPUs.}\n",
       "%          \\label{fig:gp_ddp}\n",
       "%      \\end{subfigure}\n",
       "% \\caption{(a,b) Weak scaling efficiency and (c) TeraFLOPs per second~\\textit{vs.} number of GPUs}\n",
       "%         % -- using graph parallel up to ${\\sim}50$ TFLOPs per second on 8 GPUs, and using\n",
       "%         % graph + data parallel up to ${\\sim}160$ TFLOPs per second on 256 GPUs.}\n",
       "%         \\vspace{-10pt}\n",
       "% \\end{figure}\n",
       "\n",
       "\\begin{figure}[h]\n",
       "     \\begin{subfigure}[b]{0.5\\textwidth}\n",
       "         \\centering\n",
       "         \\includegraphics[scale=0.3]{efficiency.pdf}\n",
       "         \\caption{Weak scaling efficiency.}\n",
       "        %  \\caption{Weak scaling efficiency measured by increasing the model size proportional to the number of GPUs.}\n",
       "         \\label{fig:weak_scaling}\n",
       "     \\end{subfigure}\n",
       "     \\quad\n",
       "     \\begin{subfigure}[b]{0.5\\textwidth}\n",
       "         \\centering\n",
       "         \\includegraphics[scale=0.3]{scaling.pdf}\n",
       "         \\caption{Comparison of graph and pipeline parallel training.}\n",
       "        %  \\caption{Comparing graph parallelism with pipeline parallelism. Solid blue, green and red curves show the scaling performance of graph, pipeline, and graph+pipeline parallel training. Dashed lines show the same with 32-way data parallel training. We do not show the results of training the largest models with pipeline parallelism as those runs ran out of GPU memory.}\n",
       "         \\label{fig:gp_ddp}\n",
       "     \\end{subfigure}\n",
       "\n",
       "\\caption{\\textbf{Left:} Weak scaling efficiency measured by scaling the model size proportional to the number of GPUs. \\textbf{Right:} Comparing graph parallelism with pipeline parallelism. Solid blue, green and red curves show the scaling performance of graph, pipeline, and graph+pipeline parallel training. Dashed lines show the same with 32-way data parallel training. We do not show the results of training the largest models with pipeline parallelism as those runs ran out of GPU memory.}\n",
       "% \\caption{(a,b) Weak scaling efficiency and (c) TeraFLOPs per second~\\textit{vs.} number of GPUs}\n",
       "        % -- using graph parallel up to ${\\sim}50$ TFLOPs per second on 8 GPUs, and using\n",
       "        % graph + data parallel up to ${\\sim}160$ TFLOPs per second on 256 GPUs.}\n",
       "        \\vspace{-10pt}\n",
       "\\end{figure}\n",
       "\n",
       "\\subsection{Scaling Analysis}\n",
       "\\label{sec:scaling}\n",
       "\n",
       "Weak scaling studies the effect on the throughput when computation is scaled proportional to the number of processors. We study weak scaling efficiency in terms of scaling the model size proportional to the number of GPUs. For these experiments, we use 4 different GemNet-T models, ranging from 120M parameters to nearly 1B parameters (see Table~\\ref{tab:scaling_models}).\n",
       "\n",
       "We train increasingly larger models on multiple GPUs that do not fit on a single GPU. Figure~\\ref{fig:weak_scaling} shows that we are able to obtain a scaling efficiency of roughly $79\\%$ with 8 GPUs for our largest model with nearly a billion parameters. This shows that our graph parallel training can be scaled up to billion parameter GemNet-T models while obtaining reasonably good scaling performance.\n",
       "\n",
       "Figure~\\ref{fig:weak_scaling} further shows the scaling efficiency for each model combining graph parallelism with 32-way data parallelism. Pure data parallel training with 32 GPUs obtains a scaling efficiency of $75\\%$ for the smallest model, showing the effect of network communication and load imbalance between the GPUs. Combining graph and data parallel training with 256 GPUs only reduces the scaling efficiency to $47\\%$ for the largest model compared to the 1 GPU case, suggesting the graph parallelism is promising for training extremely large models on hundreds of GPUs.\n",
       "\n",
       "Finally, figure~\\ref{fig:gp_ddp} shows the raw performance of running these models on V100 GPUs in terms of TeraFLOPs per second as a function of the number of GPUs. On a single GPU, the 120M parameter GemNet-T sustains 32 TeraFLOPs or roughly $25\\%$ of the theoretical peak FLOPS for a single GPU, and is thus a strong baseline. With 256 GPUs, the largest model sustains $3.5$ PetaFLOPs.\n",
       "\n",
       "Figure~\\ref{fig:gp_ddp} compares graph and pipeline parallelism~\\citep{huang2019gpipe} showing that graph parallelism outperforms pipeline parallelism for the models that we consider. Since each graph in the training data contains different numbers of nodes, edges and triplets, load balancing across GPUs is difficult for pipeline parallelism. Graph parallelism is able to overcome this problem since the nodes, edges and triplets of a given batch are always distributed evenly across the GPUs, helping it outperform pipeline parallelism.\n",
       "%\n",
       "It is possible, however, that pipeline parallelism might outperform graph parallelism for very deep GNNs since inter-GPU communication overhead for pipeline parallelism is independent of the number of blocks.\n",
       "Figure ~\\ref{fig:gp_ddp} also shows results with graph and pipeline parallelism combined,\n",
       "indicating that these methods are complementary to each other.\n",
       "%While training deeper GNNs, this allows one to make trade-offs between load balancing and number of inter-GPU communication steps.\n",
       "\n",
       "% \\textbf{Weak scaling}\n",
       "% % In this section, we analyze the scaling behavior of our models in terms of weak scaling efficiency.\n",
       "% studies the effect on the throughput when computation is scaled proportional to the number of processors.\n",
       "% We study weak scaling efficiency in terms of scaling batch size as well as scaling model size, both with graph parallelism.\n",
       "% For these experiments, we use 4 different GemNet-T models, ranging from 125.4 million parameters to 1.12 billion parameters (see Table~\\ref{tab:scaling_models}).\n",
       "\n",
       "% For weak scaling efficiency by batch size, we train the smallest GemNet-T model from Table~\\ref{tab:scaling_models}\n",
       "% and scale up batch size linearly with number of GPUs.\n",
       "% %\n",
       "% \\ad{Figure \\ref{fig:scaling_batch} shows that using graph parallelism to scale to 8x the batch size on 8 GPUs achieves ${\\sim}46\\%$ of linear scaling.\n",
       "%\n",
       "% For reference, if the throughput (time per batch) with 8x the batch size on 8 GPUs was the same as 1x the batch size on 1 GPU, efficiency would be $100\\%$.\n",
       "% %\n",
       "% In this case, since the model fits on a single GPU, one can alternatively use data parallelism and we expect it\n",
       "% to have better scaling efficiency with batch size because of significantly fewer quantities being communicated across GPUs than graph parallelism.}\n",
       "% we obtain roughly $46\\%$ scaling efficiency with 8 GPUs.\n",
       "\n",
       "% For weak scaling efficiency by model size, we train increasingly larger models on multiple GPUs\n",
       "% that do not fit on a single GPU.\n",
       "%\n",
       "% Figure~\\ref{fig:scaling_model} shows that we are able to obtain a scaling efficiency of roughly $50\\%$ with 8 GPUs for our largest model with 1.12B parameters.\n",
       "% %\n",
       "% \\ad{Here, data parallelism is not even applicable since this model does not fit on 1 GPU.\n",
       "% %\n",
       "% Further, we see that going from 500M parameters (on 4 GPUs) to 1.12B parameters (on 8 GPUs), scaling efficiency only decreases from\n",
       "% ${\\sim}54\\%$ to ${\\sim}50\\%$, suggesting that graph parallelism is promising in this large model size regime.}\n",
       "\n",
       "% \\ad{Finally, Figure~\\ref{fig:gp_ddp} shows that combining graph and data parallelism follows a similar linear\n",
       "% scaling behavior as graph parallel alone, suggesting that the two approaches are complementary.}\n",
       "\n",
       "\n",
       "\\section{Related Work}\n",
       "\\label{sec:related}\n",
       "\n",
       "\\paragraph{GNNs for simluating atomic systems}\n",
       "Many GNN based approaches have been proposed for the task of estimating atomic properties such as~\\citep{schutt2017quantum,gilmer2017neural,jorgensen2018neural,schutt2017schnet,schutt2018schnet,xie2018crystal,qiao2020orbnet,klicpera2020directional}, where the atoms are represented by nodes and neighboring atoms are connected by edges. An early approach for force estimation was the SchNet model \\cite{schutt2017schnet}, which computed forces using only the distance between atoms without the use of angular information. SchNet proposed the use of differentiable edge filters which enabled constructing energy-conserving models by estimating forces as the gradient of the energy.\n",
       "Subsequent work~\\citep{klicpera_dimenetpp_2020,klicpera2020directional,klicpera2021gemnet,liu2021spherical} has extended on the SchNet model by adding bond angles and dihedral angles, which has resulted in improved performance. These models make use of higher order interactions among nodes which make them highly compute and memory intensive. An alternate approach for estimating forces is to directly regress the forces as an output of the network. While this approach does not enforce energy conservation or rotational equivariance, recent work~\\citep{hu2021forcenet} has shown that such models can still accurately predict forces.\n",
       "\n",
       "\\textbf{Distributed GNN Training}.\n",
       "Research on distributed GNN training has focused on the regime of training small models on a single, large graph, for instance\n",
       "by sampling local neighborhoods around nodes to create mini-batches~\\citep{jangda2021accelerating,zhang2020agl,zhu2019aligraph}.\n",
       "%\n",
       "While these approaches can scale to very large graphs,\n",
       "they are not suitable for the task of modeling atomic systems where it is important to consider the entire graph for predicting the energy and forces.\n",
       "\n",
       "An alternate line of work, that is more similar to ours, keeps the entire graph in memory by efficiently partitioning the graph among multiple nodes \\citep{Jia2020ImprovingTA,Ma2019NeuGraphPD,tripathy2020reducing}. These methods still operate in a single graph regime that is partitioned ahead of time. %\\bw{ \"Thus, the focus of that work is on finding efficient partitions of the graph, which is not applicable to our problem since we operate on millions of graphs.\"}\n",
       "Thus the focus of that work is on finding efficient partitions of the graph, which is not applicable to our problem since we operate on millions of graphs.\n",
       "Further, these works do not train very large GNNs, or GNNs that operate on higher-order interactions (\\textit{e.g.} triplets), that are important for atomic systems.\n",
       "\n",
       "\n",
       "\\textbf{Model Parallelism}.\n",
       "methods focus on training large models that do not fit entirely on one GPU (even with a batch size of 1).\n",
       "%\n",
       "GPipe~\\citep{huang2019gpipe} splits different sequences of layers into different processors, and splits each training mini-batch into micro-batches to avoid idle time per GPU.\n",
       "%\n",
       "% GPipe~\\citep{huang2019gpipe} splits up deep networks depth-wise into several partitions of consecutive layers that are placed on different GPUs. During training, mini-batches are split up into micro-batches and pipelining is used to overlap communication with computation.\n",
       "Megatron-LM~\\citep{shoeybi2020megatronlm} splits the model breadth-wise, where Transformer layer weights are partitioned\n",
       "across multiple GPUs to distribute computation.\n",
       "%\n",
       "We see model and graph parallelism as complementary approaches that can be combined to train even larger models.\n",
       "% We leave this for future work.\n",
       "%\n",
       "\\section{Discussion}\n",
       "%\n",
       "We presented Graph Parallelism, a new method for training large GNNs for modeling atomic\n",
       "simulations, where modeling higher-order interactions between atoms (triplets / quadruplets) is critical.\n",
       "%\n",
       "We showed that training larger DimeNet++ and GemNet-T models can yield significant improvements on the OC20 dataset.\n",
       "%\n",
       "Although we demonstrated graph parallelism for just two GNNs, it is broadly applicable to a host of message-passing GNNs, including equivariant networks,\n",
       "that can be cast in the GN / EGN framework (Sec.~\\ref{sec:egn}) by appropriately picking update and aggregate functions.\n",
       "% %\n",
       "% To give a couple more examples, the model proposed in~\\cite{jing_iclr21} uses geometric representations for nodes and edges that are updated through message passing. The update functions involve the use of Geometric Vector Perceptrons (GVPs) that preserve equivariance. This model can be cast in the GN framework by using an update function that includes GVPs within it.\n",
       "% %\n",
       "% \\cite{schutt_icml21} present another equivariant GNN model called PAINN, that is composed of a series of Message and Update blocks. By viewing the computation within the message block as the EdgeUpdate function, the message passing step as the EdgeAggregate function and the Update block as the NodeUpdate function, we can see that this model can also be implemented as a GN.\n",
       "%\n",
       "\n",
       "Further, it is possible to combine graph parallelism with model parallel methods such as GPipe~\\citep{huang2019gpipe}\n",
       "to train even larger models, which could yield further improvements,\n",
       "as briefly explored in Sec~\\ref{sec:scaling}.\n",
       "%\n",
       "For force-centric GNNs,\n",
       "it should be possible to use graph parallel for `breadth-wise' scaling~\\textit{i.e.}, to split higher-order computations (\\textit{e.g.} triplets)\n",
       "across GPUs, and GPipe for `depth-wise' scaling~\\textit{i.e.}, to scale to larger number of message-passing blocks,\n",
       "sequentially split across GPUs.\n",
       "%\n",
       "For energy-centric GNNs \\textit{e.g.}, DimeNet++, this combination is less obvious since these models\n",
       "require an additional backward pass through the network to compute forces as gradients of\n",
       "energy with respect to atomic positions.\n",
       "%\n",
       "Energy-centric GNNs are common for atomic systems because they enforce the physical constraint of energy conservation.\n",
       "As we demonstrate with our DimeNet++-XL experiments, graph parallelism is applicable to energy-centric GNNs.\n",
       "\n",
       "We see scaling to large model sizes as a necessary (but not sufficient) step to effectively\n",
       "model atomic simulations from large, diverse datasets of adsorbates and catalysts.\n",
       "%\n",
       "Further progress may require marrying large scale with ways to better capture 3D geometry and physical priors.\n",
       "\n",
       "% paragraph on carbon footprint\n",
       "The carbon emissions from training large deep learning models are non-trivial and the work we have presented here is no exception~\\citep{strubell_arxiv19, schwartz_arxiv19}. We estimate that training our GemNet-XL model with Tesla v100 32 GB GPUs on cloud resources in the US ranges from 3490 - 8052 kg of CO$_2$ eq. ~\\citep{lacoste2019quantifying}. In the worst case, the emissions are roughly equivalent to 16 round trip flights from Los Angeles to New York. Assuming that the training time is fixed, emissions largely depend on the carbon intensity of the energy generation in a given region and the percentage of emissions offset with other investments by the resource provider. When choosing compute resources we recommend evaluating the stated carbon offset commitments and if possible, consider running experiments in regions that have more sustainable energy generation.\n",
       "The compute resources we utilized for this paper were committed to be $100\\%$ offset.\n",
       "\n",
       "\\bibliography{graphparallel}\n",
       "\\bibliographystyle{iclr2022_conference}\n",
       "\n",
       "\\clearpage\n",
       "\n",
       "\\appendix\n",
       "\\section{Appendix}\n",
       "% You may include other additional sections here.\n",
       "\\subsection{Additional results}\n",
       "\n",
       "Tables \\ref{tab:s2ef_results_full}, \\ref{tab:is2re_results_full} and \\ref{tab:is2rs_results_full} show results from each of the four test sets for the S2EF, IS2RE and IS2RS tasks respectively. DimeNet++-XL and GemNet-XL achieve the best results for each test set along each metric.\n",
       "\n",
       "\\begin{table*}[htb]\n",
       "    % \\small\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{5pt}\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{l|cccc}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mc{4}{c}{\\textbf{S2EF Test}}\\\\\n",
       "        & Energy MAE (EV)$\\downarrow$ & Force Cos$\\uparrow$ & Force MAE (EV/A)$\\downarrow$ & EFwT$\\uparrow$\\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{ID}} \\\\\n",
       "        GemNet-T &\t0.2257\t& 0.637\t& 0.02099\t& 2.4\\% \\\\\n",
       "        SpinConv & 0.2612 & 0.5479 & 0.02689 & 0.82\\% \\\\\n",
       "        ForceNet-Large & 2.0674\t& 0.533 & 0.02782\t& 0.02\\% \\\\\n",
       "        DimeNet++-large & 29.3333 & 0.5633 & 0.02807 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.2120} & \\textbf{0.6759} & \\textbf{0.0181} & \\textbf{3.30\\%} \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Ads}} \\\\\n",
       "        GemNet-T & 0.2099 &\t0.6242 & 0.02186 & 1.15\\% \\\\\n",
       "        SpinConv & 0.2752 & 0.5345 & 0.02769 & 0.38\\% \\\\\n",
       "        ForceNet-Large & 2.4188\t& 0.5212\t& 0.02834 & 0.01\\% \\\\\n",
       "        DimeNet++-large & 30.0338 & 0.5503 &\t0.02896 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.1980} & \\textbf{0.6642} & \\textbf{0.0186} & \\textbf{1.62\\%} \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Cat}} \\\\\n",
       "        GemNet-T & 0.3403 & 0.5813 & 0.02445 & 0.93\\% \\\\\n",
       "        SpinConv & 0.3501 & 0.5187 & 0.02849 & 0.46\\% \\\\\n",
       "        ForceNet-Large & 2.0203\t& 0.4936 & 0.03089 & 0.01\\% \\\\\n",
       "        DimeNet++-Large & 30.0437\t& 0.5109\t& 0.0312 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.3083} & \\textbf{0.6306} & \\textbf{0.0206} & \\textbf{1.72\\%} \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Both}} \\\\\n",
       "        GemNet-T & 0.3937 & 0.6221 & 0.02956 & 0.3\\% \\\\\n",
       "        SpinConv & 0.4585 & 0.5554 & 0.03556 & 0.14\\% \\\\\n",
       "        Forcenet-Large &\t2.5447\t& 0.5302 & 0.03754 & 0\\% \\\\\n",
       "        DimeNet++-Large & 36.7529 & 0.5517 & 0.03705 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.362} & \\textbf{0.6704} & \\textbf{0.0245} & \\textbf{0.61\\%} \\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Full set of results on the S2EF task comparing our GemNet-XL to the top three entries on the Open Catalyst leaderboard, showing metrics from each test set. }\n",
       "    \\label{tab:s2ef_results_full}\n",
       "\\end{table*}\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    % \\small\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{l|l|cc}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{Approach}} & \\mc{2}{c}{\\textbf{IS2RE Test}}\\\\\n",
       "        & & Energy MAE (EV)$\\downarrow$ & EwT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{ID}} \\\\\n",
       "        GemNet-T & Relaxation & \t0.3901\t& 12.37\\% \\\\\n",
       "        SpinConv & Relaxation & \t0.4207\t& 9.4\\%\t\\\\\n",
       "        Noisy Nodes & Direct & 0.4776\t& 5.71\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3764} & \\textbf{13.25\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4194 & 7.52\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{OOD Ads}} \\\\\n",
       "        GemNet-T & Relaxation &\t0.3907\t& 9.11\\% \\\\\n",
       "        SpinConv & Relaxation & \t0.4381\t& 7.47\\% \\\\\n",
       "        Noisy Nodes & Direct &\t0.5646\t& 3.49\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3677} & \\textbf{10.00\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.5258 & 3.95\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{OOD Cat}} \\\\\n",
       "        GemNet-T & Relaxation &\t0.4339 &\t10.09\\% \\\\\n",
       "        SpinConv & Relaxation & 0.4579\t& 8.16\\% \\\\\n",
       "        Noisy Nodes & Direct &\t0.4932\t& 5.02\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.4022} & \\textbf{11.61\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4373 & 6.76\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{OOD Both}} \\\\\n",
       "        GemNet-T & Relaxation &\t0.3843\t& 7.87\\% \\\\\n",
       "        SpinConv & Relaxation & 0.4203\t& 6.56\\% \\\\\n",
       "        Noisy Nodes & Direct & 0.5042\t& 3.82\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3383} & \\textbf{9.65\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4665 & 4.19\\% \\\\\n",
       "\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Experimental results on the IS2RE task comparing our GemNet-XL to the top three entries on the Open Catalyst leaderboard, showing metrics from each test set. }\n",
       "    \\label{tab:is2re_results_full}\n",
       "\\end{table*}\n",
       "\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    % \\small\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{lrccc}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\textbf{Training} & \\mc{3}{c}{\\textbf{IS2RS Test}} \\\\\n",
       "        & \\textbf{Dataset} & AFbT$\\uparrow$ & ADwT$\\uparrow$ & FbT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{ID}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 33.75\\% & 59.18\\% & 2.0\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 25.65\\% & 52.45\\% & 1.0\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 21.10\\% &\t53.68\\% & 0.2\\%\t\\\\\n",
       "        DimeNet++ & S2EF 20M + MD & 21.08\\% & 48.6\\% & 0.2\\% \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD & \\textbf{40.00\\%} & 59.90\\% & \\textbf{2.4\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & 34.61 & \\textbf{62.73\\%} & \\textbf{2.4\\%} \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Ads}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 26.84\\% & 54.59\\% & 0.2\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 20.73\\% & 48.47\\% & 0.4\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 15.70\\% &\t48.87\\% & 0.0\\%\t\\\\\n",
       "        DimeNet++- & S2EF 20M + MD & 17.05\\% & 42.98\\% & 0.0\\% \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD & \\textbf{36.01\\%} & 55.68\\% & \\textbf{1.6\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & 30.32\\% & \\textbf{58.57\\%} & 0.6\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Cat}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 24.69\\% &\t58.71\\%\t& 0.4\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 20.24\\% & 50.99\\% & 0\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 15.86\\% &\t53.92\\% & 0\\%\t\\\\\n",
       "        DimeNet++- & S2EF 20M + MD & 16.43\\% & 48.19\\% & 0\\% \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD &  \\textbf{29.62\\%} & 58.43\\% & \\textbf{0.6\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & 29.33\\% & \\textbf{62.60\\%} & 0.2\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Both}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 25.11\\% &\t62.23\\%\t& 0.2\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 20.67\\% & 54.82\\% & 0.2\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 14.01\\% &\t58.03\\% & 0\\%\t\\\\\n",
       "        DimeNet++- & S2EF 20M + MD & 14.02\\% & 51.09\\% & \\textbf{0.4\\%} \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD & 28.14\\% & 62.85\\% & \\textbf{0.4\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & \\textbf{29.02\\%} & \\textbf{66.72\\%} & \\textbf{0.4\\%} \\\\\n",
       "\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Experimental results on the IS2RS task comparing our models to the top four entries on the Open Catalyst leaderboard, showing metrics for each test dataset. The DimeNet++ and DimeNet++-XL models were trained on the S2EF 20M + MD dataset, that contains additional molecular dynamics data, which has been shown to be helpful for the IS2RS task.}\n",
       "    \\label{tab:is2rs_results_full}\n",
       "\\end{table*}\n",
       "\n",
       "\\end{document}\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from TexSoup import TexSoup\n",
    "\n",
    "\n",
    "fpath = Path(\"../data/2203.09697/graphparallel.tex\")\n",
    "soup = TexSoup(fpath.read_text(encoding=\"utf-8\"))\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\\input{math_commands.tex}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = soup.find_all(\"input\")\n",
    "input = inputs[0]\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BraceGroup('math_commands.tex')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg = input.args[0]\n",
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\\documentclass{article} % For LaTeX2e\n",
       "\\usepackage{iclr2022_conference,times}\n",
       "\n",
       "% Optional math commands from https://github.com/goodfeli/dlbook_notation.\n",
       "%%%%% NEW MATH DEFINITIONS %%%%%\n",
       "\n",
       "\\usepackage{amsmath,amsfonts,bm}\n",
       "\n",
       "% Mark sections of captions for referring to divisions of figures\n",
       "\\newcommand{\\figleft}{{\\em (Left)}}\n",
       "\\newcommand{\\figcenter}{{\\em (Center)}}\n",
       "\\newcommand{\\figright}{{\\em (Right)}}\n",
       "\\newcommand{\\figtop}{{\\em (Top)}}\n",
       "\\newcommand{\\figbottom}{{\\em (Bottom)}}\n",
       "\\newcommand{\\captiona}{{\\em (a)}}\n",
       "\\newcommand{\\captionb}{{\\em (b)}}\n",
       "\\newcommand{\\captionc}{{\\em (c)}}\n",
       "\\newcommand{\\captiond}{{\\em (d)}}\n",
       "\n",
       "% Highlight a newly defined term\n",
       "\\newcommand{\\newterm}[1]{{\\bf #1}}\n",
       "\n",
       "\n",
       "% Figure reference, lower-case.\n",
       "\\def\\figref{#1}{figure~\\ref{#1}}\n",
       "% Figure reference, capital. For start of sentence\n",
       "\\def\\Figref{#1}{Figure~\\ref{#1}}\n",
       "\\def\\twofigref{#1#2}{figures \\ref{#1} and \\ref{#2}}\n",
       "\\def\\quadfigref{#1#2#3#4}{figures \\ref{#1}, \\ref{#2}, \\ref{#3} and \\ref{#4}}\n",
       "% Section reference, lower-case.\n",
       "\\def\\secref{#1}{section~\\ref{#1}}\n",
       "% Section reference, capital.\n",
       "\\def\\Secref{#1}{Section~\\ref{#1}}\n",
       "% Reference to two sections.\n",
       "\\def\\twosecrefs{#1#2}{sections \\ref{#1} and \\ref{#2}}\n",
       "% Reference to three sections.\n",
       "\\def\\secrefs{#1#2#3}{sections \\ref{#1}, \\ref{#2} and \\ref{#3}}\n",
       "% Reference to an equation, lower-case.\n",
       "\\def\\eqref{#1}{equation~\\ref{#1}}\n",
       "% Reference to an equation, upper case\n",
       "\\def\\Eqref{#1}{Equation~\\ref{#1}}\n",
       "% A raw reference to an equation---avoid using if possible\n",
       "\\def\\plaineqref{#1}{\\ref{#1}}\n",
       "% Reference to a chapter, lower-case.\n",
       "\\def\\chapref{#1}{chapter~\\ref{#1}}\n",
       "% Reference to an equation, upper case.\n",
       "\\def\\Chapref{#1}{Chapter~\\ref{#1}}\n",
       "% Reference to a range of chapters\n",
       "\\def\\rangechapref{#1#2}{chapters\\ref{#1}--\\ref{#2}}\n",
       "% Reference to an algorithm, lower-case.\n",
       "\\def\\algref{#1}{algorithm~\\ref{#1}}\n",
       "% Reference to an algorithm, upper case.\n",
       "\\def\\Algref{#1}{Algorithm~\\ref{#1}}\n",
       "\\def\\twoalgref{#1#2}{algorithms \\ref{#1} and \\ref{#2}}\n",
       "\\def\\Twoalgref{#1#2}{Algorithms \\ref{#1} and \\ref{#2}}\n",
       "% Reference to a part, lower case\n",
       "\\def\\partref{#1}{part~\\ref{#1}}\n",
       "% Reference to a part, upper case\n",
       "\\def\\Partref{#1}{Part~\\ref{#1}}\n",
       "\\def\\twopartref{#1#2}{parts \\ref{#1} and \\ref{#2}}\n",
       "\n",
       "\\def\\ceil{#1}{\\lceil #1 \\rceil}\n",
       "\\def\\floor{#1}{\\lfloor #1 \\rfloor}\n",
       "\\def{\\1}{\\bm{1}}\n",
       "\\newcommand{\\train}{\\mathcal{D}}\n",
       "\\newcommand{\\valid}{\\mathcal{D_{\\mathrm{valid}}}}\n",
       "\\newcommand{\\test}{\\mathcal{D_{\\mathrm{test}}}}\n",
       "\n",
       "\\def\\eps{{\\epsilon}}\n",
       "\n",
       "\n",
       "% Random variables\n",
       "\\def\\reta{{\\textnormal{$\\eta$}}}\n",
       "\\def\\ra{{\\textnormal{a}}}\n",
       "\\def\\rb{{\\textnormal{b}}}\n",
       "\\def\\rc{{\\textnormal{c}}}\n",
       "\\def\\rd{{\\textnormal{d}}}\n",
       "\\def\\re{{\\textnormal{e}}}\n",
       "\\def\\rf{{\\textnormal{f}}}\n",
       "\\def\\rg{{\\textnormal{g}}}\n",
       "\\def\\rh{{\\textnormal{h}}}\n",
       "\\def\\ri{{\\textnormal{i}}}\n",
       "\\def\\rj{{\\textnormal{j}}}\n",
       "\\def\\rk{{\\textnormal{k}}}\n",
       "\\def\\rl{{\\textnormal{l}}}\n",
       "% rm is already a command, just don't name any random variables m\n",
       "\\def\\rn{{\\textnormal{n}}}\n",
       "\\def\\ro{{\\textnormal{o}}}\n",
       "\\def\\rp{{\\textnormal{p}}}\n",
       "\\def\\rq{{\\textnormal{q}}}\n",
       "\\def\\rr{{\\textnormal{r}}}\n",
       "\\def\\rs{{\\textnormal{s}}}\n",
       "\\def\\rt{{\\textnormal{t}}}\n",
       "\\def\\ru{{\\textnormal{u}}}\n",
       "\\def\\rv{{\\textnormal{v}}}\n",
       "\\def\\rw{{\\textnormal{w}}}\n",
       "\\def\\rx{{\\textnormal{x}}}\n",
       "\\def\\ry{{\\textnormal{y}}}\n",
       "\\def\\rz{{\\textnormal{z}}}\n",
       "\n",
       "% Random vectors\n",
       "\\def\\rvepsilon{{\\mathbf{\\epsilon}}}\n",
       "\\def\\rvtheta{{\\mathbf{\\theta}}}\n",
       "\\def\\rva{{\\mathbf{a}}}\n",
       "\\def\\rvb{{\\mathbf{b}}}\n",
       "\\def\\rvc{{\\mathbf{c}}}\n",
       "\\def\\rvd{{\\mathbf{d}}}\n",
       "\\def\\rve{{\\mathbf{e}}}\n",
       "\\def\\rvf{{\\mathbf{f}}}\n",
       "\\def\\rvg{{\\mathbf{g}}}\n",
       "\\def\\rvh{{\\mathbf{h}}}\n",
       "\\def\\rvu{{\\mathbf{i}}}\n",
       "\\def\\rvj{{\\mathbf{j}}}\n",
       "\\def\\rvk{{\\mathbf{k}}}\n",
       "\\def\\rvl{{\\mathbf{l}}}\n",
       "\\def\\rvm{{\\mathbf{m}}}\n",
       "\\def\\rvn{{\\mathbf{n}}}\n",
       "\\def\\rvo{{\\mathbf{o}}}\n",
       "\\def\\rvp{{\\mathbf{p}}}\n",
       "\\def\\rvq{{\\mathbf{q}}}\n",
       "\\def\\rvr{{\\mathbf{r}}}\n",
       "\\def\\rvs{{\\mathbf{s}}}\n",
       "\\def\\rvt{{\\mathbf{t}}}\n",
       "\\def\\rvu{{\\mathbf{u}}}\n",
       "\\def\\rvv{{\\mathbf{v}}}\n",
       "\\def\\rvw{{\\mathbf{w}}}\n",
       "\\def\\rvx{{\\mathbf{x}}}\n",
       "\\def\\rvy{{\\mathbf{y}}}\n",
       "\\def\\rvz{{\\mathbf{z}}}\n",
       "\n",
       "% Elements of random vectors\n",
       "\\def\\erva{{\\textnormal{a}}}\n",
       "\\def\\ervb{{\\textnormal{b}}}\n",
       "\\def\\ervc{{\\textnormal{c}}}\n",
       "\\def\\ervd{{\\textnormal{d}}}\n",
       "\\def\\erve{{\\textnormal{e}}}\n",
       "\\def\\ervf{{\\textnormal{f}}}\n",
       "\\def\\ervg{{\\textnormal{g}}}\n",
       "\\def\\ervh{{\\textnormal{h}}}\n",
       "\\def\\ervi{{\\textnormal{i}}}\n",
       "\\def\\ervj{{\\textnormal{j}}}\n",
       "\\def\\ervk{{\\textnormal{k}}}\n",
       "\\def\\ervl{{\\textnormal{l}}}\n",
       "\\def\\ervm{{\\textnormal{m}}}\n",
       "\\def\\ervn{{\\textnormal{n}}}\n",
       "\\def\\ervo{{\\textnormal{o}}}\n",
       "\\def\\ervp{{\\textnormal{p}}}\n",
       "\\def\\ervq{{\\textnormal{q}}}\n",
       "\\def\\ervr{{\\textnormal{r}}}\n",
       "\\def\\ervs{{\\textnormal{s}}}\n",
       "\\def\\ervt{{\\textnormal{t}}}\n",
       "\\def\\ervu{{\\textnormal{u}}}\n",
       "\\def\\ervv{{\\textnormal{v}}}\n",
       "\\def\\ervw{{\\textnormal{w}}}\n",
       "\\def\\ervx{{\\textnormal{x}}}\n",
       "\\def\\ervy{{\\textnormal{y}}}\n",
       "\\def\\ervz{{\\textnormal{z}}}\n",
       "\n",
       "% Random matrices\n",
       "\\def\\rmA{{\\mathbf{A}}}\n",
       "\\def\\rmB{{\\mathbf{B}}}\n",
       "\\def\\rmC{{\\mathbf{C}}}\n",
       "\\def\\rmD{{\\mathbf{D}}}\n",
       "\\def\\rmE{{\\mathbf{E}}}\n",
       "\\def\\rmF{{\\mathbf{F}}}\n",
       "\\def\\rmG{{\\mathbf{G}}}\n",
       "\\def\\rmH{{\\mathbf{H}}}\n",
       "\\def\\rmI{{\\mathbf{I}}}\n",
       "\\def\\rmJ{{\\mathbf{J}}}\n",
       "\\def\\rmK{{\\mathbf{K}}}\n",
       "\\def\\rmL{{\\mathbf{L}}}\n",
       "\\def\\rmM{{\\mathbf{M}}}\n",
       "\\def\\rmN{{\\mathbf{N}}}\n",
       "\\def\\rmO{{\\mathbf{O}}}\n",
       "\\def\\rmP{{\\mathbf{P}}}\n",
       "\\def\\rmQ{{\\mathbf{Q}}}\n",
       "\\def\\rmR{{\\mathbf{R}}}\n",
       "\\def\\rmS{{\\mathbf{S}}}\n",
       "\\def\\rmT{{\\mathbf{T}}}\n",
       "\\def\\rmU{{\\mathbf{U}}}\n",
       "\\def\\rmV{{\\mathbf{V}}}\n",
       "\\def\\rmW{{\\mathbf{W}}}\n",
       "\\def\\rmX{{\\mathbf{X}}}\n",
       "\\def\\rmY{{\\mathbf{Y}}}\n",
       "\\def\\rmZ{{\\mathbf{Z}}}\n",
       "\n",
       "% Elements of random matrices\n",
       "\\def\\ermA{{\\textnormal{A}}}\n",
       "\\def\\ermB{{\\textnormal{B}}}\n",
       "\\def\\ermC{{\\textnormal{C}}}\n",
       "\\def\\ermD{{\\textnormal{D}}}\n",
       "\\def\\ermE{{\\textnormal{E}}}\n",
       "\\def\\ermF{{\\textnormal{F}}}\n",
       "\\def\\ermG{{\\textnormal{G}}}\n",
       "\\def\\ermH{{\\textnormal{H}}}\n",
       "\\def\\ermI{{\\textnormal{I}}}\n",
       "\\def\\ermJ{{\\textnormal{J}}}\n",
       "\\def\\ermK{{\\textnormal{K}}}\n",
       "\\def\\ermL{{\\textnormal{L}}}\n",
       "\\def\\ermM{{\\textnormal{M}}}\n",
       "\\def\\ermN{{\\textnormal{N}}}\n",
       "\\def\\ermO{{\\textnormal{O}}}\n",
       "\\def\\ermP{{\\textnormal{P}}}\n",
       "\\def\\ermQ{{\\textnormal{Q}}}\n",
       "\\def\\ermR{{\\textnormal{R}}}\n",
       "\\def\\ermS{{\\textnormal{S}}}\n",
       "\\def\\ermT{{\\textnormal{T}}}\n",
       "\\def\\ermU{{\\textnormal{U}}}\n",
       "\\def\\ermV{{\\textnormal{V}}}\n",
       "\\def\\ermW{{\\textnormal{W}}}\n",
       "\\def\\ermX{{\\textnormal{X}}}\n",
       "\\def\\ermY{{\\textnormal{Y}}}\n",
       "\\def\\ermZ{{\\textnormal{Z}}}\n",
       "\n",
       "% Vectors\n",
       "\\def\\vzero{{\\bm{0}}}\n",
       "\\def\\vone{{\\bm{1}}}\n",
       "\\def\\vmu{{\\bm{\\mu}}}\n",
       "\\def\\vtheta{{\\bm{\\theta}}}\n",
       "\\def\\va{{\\bm{a}}}\n",
       "\\def\\vb{{\\bm{b}}}\n",
       "\\def\\vc{{\\bm{c}}}\n",
       "\\def\\vd{{\\bm{d}}}\n",
       "\\def\\ve{{\\bm{e}}}\n",
       "\\def\\vf{{\\bm{f}}}\n",
       "\\def\\vg{{\\bm{g}}}\n",
       "\\def\\vh{{\\bm{h}}}\n",
       "\\def\\vi{{\\bm{i}}}\n",
       "\\def\\vj{{\\bm{j}}}\n",
       "\\def\\vk{{\\bm{k}}}\n",
       "\\def\\vl{{\\bm{l}}}\n",
       "\\def\\vm{{\\bm{m}}}\n",
       "\\def\\vn{{\\bm{n}}}\n",
       "\\def\\vo{{\\bm{o}}}\n",
       "\\def\\vp{{\\bm{p}}}\n",
       "\\def\\vq{{\\bm{q}}}\n",
       "\\def\\vr{{\\bm{r}}}\n",
       "\\def\\vs{{\\bm{s}}}\n",
       "\\def\\vt{{\\bm{t}}}\n",
       "\\def\\vu{{\\bm{u}}}\n",
       "\\def\\vv{{\\bm{v}}}\n",
       "\\def\\vw{{\\bm{w}}}\n",
       "\\def\\vx{{\\bm{x}}}\n",
       "\\def\\vy{{\\bm{y}}}\n",
       "\\def\\vz{{\\bm{z}}}\n",
       "\n",
       "% Elements of vectors\n",
       "\\def\\evalpha{{\\alpha}}\n",
       "\\def\\evbeta{{\\beta}}\n",
       "\\def\\evepsilon{{\\epsilon}}\n",
       "\\def\\evlambda{{\\lambda}}\n",
       "\\def\\evomega{{\\omega}}\n",
       "\\def\\evmu{{\\mu}}\n",
       "\\def\\evpsi{{\\psi}}\n",
       "\\def\\evsigma{{\\sigma}}\n",
       "\\def\\evtheta{{\\theta}}\n",
       "\\def\\eva{{a}}\n",
       "\\def\\evb{{b}}\n",
       "\\def\\evc{{c}}\n",
       "\\def\\evd{{d}}\n",
       "\\def\\eve{{e}}\n",
       "\\def\\evf{{f}}\n",
       "\\def\\evg{{g}}\n",
       "\\def\\evh{{h}}\n",
       "\\def\\evi{{i}}\n",
       "\\def\\evj{{j}}\n",
       "\\def\\evk{{k}}\n",
       "\\def\\evl{{l}}\n",
       "\\def\\evm{{m}}\n",
       "\\def\\evn{{n}}\n",
       "\\def\\evo{{o}}\n",
       "\\def\\evp{{p}}\n",
       "\\def\\evq{{q}}\n",
       "\\def\\evr{{r}}\n",
       "\\def\\evs{{s}}\n",
       "\\def\\evt{{t}}\n",
       "\\def\\evu{{u}}\n",
       "\\def\\evv{{v}}\n",
       "\\def\\evw{{w}}\n",
       "\\def\\evx{{x}}\n",
       "\\def\\evy{{y}}\n",
       "\\def\\evz{{z}}\n",
       "\n",
       "% Matrix\n",
       "\\def\\mA{{\\bm{A}}}\n",
       "\\def\\mB{{\\bm{B}}}\n",
       "\\def\\mC{{\\bm{C}}}\n",
       "\\def\\mD{{\\bm{D}}}\n",
       "\\def\\mE{{\\bm{E}}}\n",
       "\\def\\mF{{\\bm{F}}}\n",
       "\\def\\mG{{\\bm{G}}}\n",
       "\\def\\mH{{\\bm{H}}}\n",
       "\\def\\mI{{\\bm{I}}}\n",
       "\\def\\mJ{{\\bm{J}}}\n",
       "\\def\\mK{{\\bm{K}}}\n",
       "\\def\\mL{{\\bm{L}}}\n",
       "\\def\\mM{{\\bm{M}}}\n",
       "\\def\\mN{{\\bm{N}}}\n",
       "\\def\\mO{{\\bm{O}}}\n",
       "\\def\\mP{{\\bm{P}}}\n",
       "\\def\\mQ{{\\bm{Q}}}\n",
       "\\def\\mR{{\\bm{R}}}\n",
       "\\def\\mS{{\\bm{S}}}\n",
       "\\def\\mT{{\\bm{T}}}\n",
       "\\def\\mU{{\\bm{U}}}\n",
       "\\def\\mV{{\\bm{V}}}\n",
       "\\def\\mW{{\\bm{W}}}\n",
       "\\def\\mX{{\\bm{X}}}\n",
       "\\def\\mY{{\\bm{Y}}}\n",
       "\\def\\mZ{{\\bm{Z}}}\n",
       "\\def\\mBeta{{\\bm{\\beta}}}\n",
       "\\def\\mPhi{{\\bm{\\Phi}}}\n",
       "\\def\\mLambda{{\\bm{\\Lambda}}}\n",
       "\\def\\mSigma{{\\bm{\\Sigma}}}\n",
       "\n",
       "% Tensor\n",
       "\\DeclareMathAlphabet{\\mathsfit}{\\encodingdefault}{\\sfdefault}{m}{sl}\n",
       "\\SetMathAlphabet{\\mathsfit}{bold}{\\encodingdefault}{\\sfdefault}{bx}{n}\n",
       "\\newcommand{\\tens}[1]{\\bm{\\mathsfit{#1}}}\n",
       "\\def\\tA{{\\tens{A}}}\n",
       "\\def\\tB{{\\tens{B}}}\n",
       "\\def\\tC{{\\tens{C}}}\n",
       "\\def\\tD{{\\tens{D}}}\n",
       "\\def\\tE{{\\tens{E}}}\n",
       "\\def\\tF{{\\tens{F}}}\n",
       "\\def\\tG{{\\tens{G}}}\n",
       "\\def\\tH{{\\tens{H}}}\n",
       "\\def\\tI{{\\tens{I}}}\n",
       "\\def\\tJ{{\\tens{J}}}\n",
       "\\def\\tK{{\\tens{K}}}\n",
       "\\def\\tL{{\\tens{L}}}\n",
       "\\def\\tM{{\\tens{M}}}\n",
       "\\def\\tN{{\\tens{N}}}\n",
       "\\def\\tO{{\\tens{O}}}\n",
       "\\def\\tP{{\\tens{P}}}\n",
       "\\def\\tQ{{\\tens{Q}}}\n",
       "\\def\\tR{{\\tens{R}}}\n",
       "\\def\\tS{{\\tens{S}}}\n",
       "\\def\\tT{{\\tens{T}}}\n",
       "\\def\\tU{{\\tens{U}}}\n",
       "\\def\\tV{{\\tens{V}}}\n",
       "\\def\\tW{{\\tens{W}}}\n",
       "\\def\\tX{{\\tens{X}}}\n",
       "\\def\\tY{{\\tens{Y}}}\n",
       "\\def\\tZ{{\\tens{Z}}}\n",
       "\n",
       "\n",
       "% Graph\n",
       "\\def\\gA{{\\mathcal{A}}}\n",
       "\\def\\gB{{\\mathcal{B}}}\n",
       "\\def\\gC{{\\mathcal{C}}}\n",
       "\\def\\gD{{\\mathcal{D}}}\n",
       "\\def\\gE{{\\mathcal{E}}}\n",
       "\\def\\gF{{\\mathcal{F}}}\n",
       "\\def\\gG{{\\mathcal{G}}}\n",
       "\\def\\gH{{\\mathcal{H}}}\n",
       "\\def\\gI{{\\mathcal{I}}}\n",
       "\\def\\gJ{{\\mathcal{J}}}\n",
       "\\def\\gK{{\\mathcal{K}}}\n",
       "\\def\\gL{{\\mathcal{L}}}\n",
       "\\def\\gM{{\\mathcal{M}}}\n",
       "\\def\\gN{{\\mathcal{N}}}\n",
       "\\def\\gO{{\\mathcal{O}}}\n",
       "\\def\\gP{{\\mathcal{P}}}\n",
       "\\def\\gQ{{\\mathcal{Q}}}\n",
       "\\def\\gR{{\\mathcal{R}}}\n",
       "\\def\\gS{{\\mathcal{S}}}\n",
       "\\def\\gT{{\\mathcal{T}}}\n",
       "\\def\\gU{{\\mathcal{U}}}\n",
       "\\def\\gV{{\\mathcal{V}}}\n",
       "\\def\\gW{{\\mathcal{W}}}\n",
       "\\def\\gX{{\\mathcal{X}}}\n",
       "\\def\\gY{{\\mathcal{Y}}}\n",
       "\\def\\gZ{{\\mathcal{Z}}}\n",
       "\n",
       "% Sets\n",
       "\\def\\sA{{\\mathbb{A}}}\n",
       "\\def\\sB{{\\mathbb{B}}}\n",
       "\\def\\sC{{\\mathbb{C}}}\n",
       "\\def\\sD{{\\mathbb{D}}}\n",
       "% Don't use a set called E, because this would be the same as our symbol\n",
       "% for expectation.\n",
       "\\def\\sF{{\\mathbb{F}}}\n",
       "\\def\\sG{{\\mathbb{G}}}\n",
       "\\def\\sH{{\\mathbb{H}}}\n",
       "\\def\\sI{{\\mathbb{I}}}\n",
       "\\def\\sJ{{\\mathbb{J}}}\n",
       "\\def\\sK{{\\mathbb{K}}}\n",
       "\\def\\sL{{\\mathbb{L}}}\n",
       "\\def\\sM{{\\mathbb{M}}}\n",
       "\\def\\sN{{\\mathbb{N}}}\n",
       "\\def\\sO{{\\mathbb{O}}}\n",
       "\\def\\sP{{\\mathbb{P}}}\n",
       "\\def\\sQ{{\\mathbb{Q}}}\n",
       "\\def\\sR{{\\mathbb{R}}}\n",
       "\\def\\sS{{\\mathbb{S}}}\n",
       "\\def\\sT{{\\mathbb{T}}}\n",
       "\\def\\sU{{\\mathbb{U}}}\n",
       "\\def\\sV{{\\mathbb{V}}}\n",
       "\\def\\sW{{\\mathbb{W}}}\n",
       "\\def\\sX{{\\mathbb{X}}}\n",
       "\\def\\sY{{\\mathbb{Y}}}\n",
       "\\def\\sZ{{\\mathbb{Z}}}\n",
       "\n",
       "% Entries of a matrix\n",
       "\\def\\emLambda{{\\Lambda}}\n",
       "\\def\\emA{{A}}\n",
       "\\def\\emB{{B}}\n",
       "\\def\\emC{{C}}\n",
       "\\def\\emD{{D}}\n",
       "\\def\\emE{{E}}\n",
       "\\def\\emF{{F}}\n",
       "\\def\\emG{{G}}\n",
       "\\def\\emH{{H}}\n",
       "\\def\\emI{{I}}\n",
       "\\def\\emJ{{J}}\n",
       "\\def\\emK{{K}}\n",
       "\\def\\emL{{L}}\n",
       "\\def\\emM{{M}}\n",
       "\\def\\emN{{N}}\n",
       "\\def\\emO{{O}}\n",
       "\\def\\emP{{P}}\n",
       "\\def\\emQ{{Q}}\n",
       "\\def\\emR{{R}}\n",
       "\\def\\emS{{S}}\n",
       "\\def\\emT{{T}}\n",
       "\\def\\emU{{U}}\n",
       "\\def\\emV{{V}}\n",
       "\\def\\emW{{W}}\n",
       "\\def\\emX{{X}}\n",
       "\\def\\emY{{Y}}\n",
       "\\def\\emZ{{Z}}\n",
       "\\def\\emSigma{{\\Sigma}}\n",
       "\n",
       "% entries of a tensor\n",
       "% Same font as tensor, without \\bm wrapper\n",
       "\\newcommand{\\etens}[1]{\\mathsfit{#1}}\n",
       "\\def\\etLambda{{\\etens{\\Lambda}}}\n",
       "\\def\\etA{{\\etens{A}}}\n",
       "\\def\\etB{{\\etens{B}}}\n",
       "\\def\\etC{{\\etens{C}}}\n",
       "\\def\\etD{{\\etens{D}}}\n",
       "\\def\\etE{{\\etens{E}}}\n",
       "\\def\\etF{{\\etens{F}}}\n",
       "\\def\\etG{{\\etens{G}}}\n",
       "\\def\\etH{{\\etens{H}}}\n",
       "\\def\\etI{{\\etens{I}}}\n",
       "\\def\\etJ{{\\etens{J}}}\n",
       "\\def\\etK{{\\etens{K}}}\n",
       "\\def\\etL{{\\etens{L}}}\n",
       "\\def\\etM{{\\etens{M}}}\n",
       "\\def\\etN{{\\etens{N}}}\n",
       "\\def\\etO{{\\etens{O}}}\n",
       "\\def\\etP{{\\etens{P}}}\n",
       "\\def\\etQ{{\\etens{Q}}}\n",
       "\\def\\etR{{\\etens{R}}}\n",
       "\\def\\etS{{\\etens{S}}}\n",
       "\\def\\etT{{\\etens{T}}}\n",
       "\\def\\etU{{\\etens{U}}}\n",
       "\\def\\etV{{\\etens{V}}}\n",
       "\\def\\etW{{\\etens{W}}}\n",
       "\\def\\etX{{\\etens{X}}}\n",
       "\\def\\etY{{\\etens{Y}}}\n",
       "\\def\\etZ{{\\etens{Z}}}\n",
       "\n",
       "% The true underlying data generating distribution\n",
       "\\newcommand{\\pdata}{p_{\\rm{data}}}\n",
       "% The empirical distribution defined by the training set\n",
       "\\newcommand{\\ptrain}{\\hat{p}_{\\rm{data}}}\n",
       "\\newcommand{\\Ptrain}{\\hat{P}_{\\rm{data}}}\n",
       "% The model distribution\n",
       "\\newcommand{\\pmodel}{p_{\\rm{model}}}\n",
       "\\newcommand{\\Pmodel}{P_{\\rm{model}}}\n",
       "\\newcommand{\\ptildemodel}{\\tilde{p}_{\\rm{model}}}\n",
       "% Stochastic autoencoder distributions\n",
       "\\newcommand{\\pencode}{p_{\\rm{encoder}}}\n",
       "\\newcommand{\\pdecode}{p_{\\rm{decoder}}}\n",
       "\\newcommand{\\precons}{p_{\\rm{reconstruct}}}\n",
       "\n",
       "\\newcommand{\\laplace}{\\mathrm{Laplace}} % Laplace distribution\n",
       "\n",
       "\\newcommand{\\E}{\\mathbb{E}}\n",
       "\\newcommand{\\Ls}{\\mathcal{L}}\n",
       "\\newcommand{\\R}{\\mathbb{R}}\n",
       "\\newcommand{\\emp}{\\tilde{p}}\n",
       "\\newcommand{\\lr}{\\alpha}\n",
       "\\newcommand{\\reg}{\\lambda}\n",
       "\\newcommand{\\rect}{\\mathrm{rectifier}}\n",
       "\\newcommand{\\softmax}{\\mathrm{softmax}}\n",
       "\\newcommand{\\sigmoid}{\\sigma}\n",
       "\\newcommand{\\softplus}{\\zeta}\n",
       "\\newcommand{\\KL}{D_{\\mathrm{KL}}}\n",
       "\\newcommand{\\Var}{\\mathrm{Var}}\n",
       "\\newcommand{\\standarderror}{\\mathrm{SE}}\n",
       "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
       "% Wolfram Mathworld says $L^2$ is for function spaces and $\\ell^2$ is for vectors\n",
       "% But then they seem to use $L^2$ for vectors throughout the site, and so does\n",
       "% wikipedia.\n",
       "\\newcommand{\\normlzero}{L^0}\n",
       "\\newcommand{\\normlone}{L^1}\n",
       "\\newcommand{\\normltwo}{L^2}\n",
       "\\newcommand{\\normlp}{L^p}\n",
       "\\newcommand{\\normmax}{L^\\infty}\n",
       "\n",
       "\\newcommand{\\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.\n",
       "\n",
       "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
       "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
       "\n",
       "\\DeclareMathOperator{\\sign}{sign}\n",
       "\\DeclareMathOperator{\\Tr}{Tr}\n",
       "\\let\\ab\\allowbreak\n",
       "\n",
       "\n",
       "% font management\n",
       "\\usepackage{relsize}\n",
       "\\usepackage[T1]{fontenc}\n",
       "\\usepackage[scaled=0.90]{inconsolata}\n",
       "\n",
       "\\usepackage{hyperref}\n",
       "\\usepackage{url}\n",
       "\\usepackage{xcolor}\n",
       "\\usepackage{booktabs}\n",
       "\\usepackage{multirow}\n",
       "\\usepackage{hhline}\n",
       "% \\usepackage[table]{xcolor}\n",
       "\\usepackage{caption}\n",
       "\\usepackage{subcaption}\n",
       "\\usepackage{grffile}\n",
       "\n",
       "\\definecolor{Belize}{RGB}{41,128,185}\n",
       "\n",
       "\\newcommand{\\mr}[2]{\\multirow{#1}{*}{#2}}\n",
       "\\newcommand{\\mc}[3]{\\multicolumn{#1}{#2}{#3}}\n",
       "\n",
       "\\newcommand{\\xx}{\\mathbf{x}}\n",
       "\\newcommand{\\ff}{\\mathbf{f}}\n",
       "\\newcommand{\\zz}{\\mathbf{z}}\n",
       "\\newcommand{\\FF}{\\mathbf{F}}\n",
       "\\newcommand{\\GGG}{\\mathbf{G}}\n",
       "\\newcommand{\\GG}{\\mathbf{GNN}}\n",
       "\\newcommand{\\uu}{\\mathbf{u}}\n",
       "% \\newcommand{\\vv}{\\mathbf{v}}\n",
       "\\newcommand{\\ee}{\\mathbf{e}}\n",
       "\\newcommand{\\hh}{\\mathbf{h}}\n",
       "\\newcommand{\\ttt}{\\mathbf{t}}\n",
       "\n",
       "\\newcommand{\\mm}{\\mathbf{m}}\n",
       "\\newcommand{\\aaa}{\\mathbf{a}}\n",
       "\n",
       "\\newcommand{\\N}{\\mathcal{N}}\n",
       "\n",
       "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
       "\\newcommand{\\lz}[1]{\\textcolor{blue}{#1}}\n",
       "\\newcommand{\\ad}[1]{\\textcolor{Belize}{#1}}\n",
       "\\newcommand{\\bw}[1]{\\textcolor{purple}{#1}}\n",
       "\\newcommand{\\as}[1]{\\textcolor{brown}{#1}}\n",
       "\\newcommand{\\TODO}[1]{\\textcolor{red}{TODO: #1}}\n",
       "\n",
       "\\DeclareMathOperator{\\F}{f}\n",
       "\\DeclareMathOperator{\\Fint}{f_{int}}\n",
       "\\DeclareMathOperator{\\Fupdate}{f_{update}}\n",
       "\\DeclareMathOperator{\\TU}{TripletUpdate}\n",
       "\\DeclareMathOperator{\\EU}{EdgeUpdate}\n",
       "\\DeclareMathOperator{\\NU}{NodeUpdate}\n",
       "\\DeclareMathOperator{\\GU}{GlobalUpdate}\n",
       "\n",
       "\\DeclareMathOperator{\\TA}{TripletAggr}\n",
       "\\DeclareMathOperator{\\EA}{EdgeAggr}\n",
       "\\DeclareMathOperator{\\NA}{NodeAggr}\n",
       "\\DeclareMathOperator{\\GA}{GlobalAggr}\n",
       "\n",
       "\\DeclareMathOperator{\\AllReduce}{AllReduce}\n",
       "\n",
       "\n",
       "\\title{Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations}\n",
       "\n",
       "% Authors must not appear in the submitted version. They should be hidden\n",
       "% as long as the \\iclrfinalcopy macro remains commented out below.\n",
       "% Non-anonymous submissions will be rejected without review.\n",
       "\n",
       "\\author{Anuroop Sriram$^\\dagger$, Abhishek Das$^\\dagger$, Brandon M. Wood$^{\\ddagger \\rightarrow \\dagger}$, Siddharth Goyal$^\\dagger$, C. Lawrence Zitnick$^\\dagger$ \\\\\n",
       "$^\\dagger$Meta FAIR \\\\\n",
       "$^\\ddagger$National Energy Research Scientific Computing Center (NERSC) \\\\\n",
       "\\texttt{\\{anuroops,abhshkdz,bmwood,sidgoyal,zitnick\\}@fb.com} \\\\\n",
       "% \\And\n",
       "% Ji Q. Ren \\& Yevgeny LeNet \\\\\n",
       "% Department of Computational Neuroscience \\\\\n",
       "% University of the Witwatersrand \\\\\n",
       "% Joburg, South Africa \\\\\n",
       "% \\texttt{\\{robot,net\\}@wits.ac.za} \\\\\n",
       "% \\AND\n",
       "% Coauthor \\\\\n",
       "% Affiliation \\\\\n",
       "% Address \\\\\n",
       "% \\texttt{email}\n",
       "}\n",
       "\n",
       "% The \\author macro works with any number of authors. There are two commands\n",
       "% used to separate the names and addresses of multiple authors: \\And and \\AND.\n",
       "%\n",
       "% Using \\And between authors leaves it to \\LaTeX{} to determine where to break\n",
       "% the lines. Using \\AND forces a linebreak at that point. So, if \\LaTeX{}\n",
       "% puts 3 of 4 authors names on the first line, and the last on the second\n",
       "% line, try using \\AND instead of \\And before the third author name.\n",
       "\n",
       "\\newcommand{\\fix}{\\marginpar{FIX}}\n",
       "\\newcommand{\\new}{\\marginpar{NEW}}\n",
       "\n",
       "\\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.\n",
       "\\begin{document}\n",
       "\n",
       "\\maketitle\n",
       "\n",
       "\\begin{abstract}\n",
       "\n",
       "Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations\n",
       "has the potential to revolutionize catalyst discovery, which is a key step in\n",
       "making progress towards the energy breakthroughs needed to combat climate change.\n",
       "However, the GNNs that have proven most effective for this task are memory\n",
       "intensive as they model higher-order interactions in the graphs such as those\n",
       "between triplets or quadruplets of atoms, making it challenging to scale these models.\n",
       "In this paper, we introduce \\emph{Graph Parallelism}, a method to distribute input\n",
       "graphs across multiple GPUs, enabling us to train very large GNNs with hundreds\n",
       "of millions or billions of parameters. We empirically evaluate our method by\n",
       "scaling up the number of parameters of the recently proposed DimeNet++ %~\\citep{klicpera_dimenetpp_2020}\n",
       "and\n",
       "GemNet %~\\citep{klicpera2021gemnet} \\bw{is it common to have citations in the abstract?}\n",
       "models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset%~\\citep{OC20}\n",
       ",\n",
       "these graph-parallelized models lead to relative improvements of 1) $15\\%$ on\n",
       "the force MAE metric for the S2EF task and 2) $21\\%$ on the AFbT metric for the\n",
       "IS2RS task, establishing new state-of-the-art results.\n",
       "\n",
       "\\end{abstract}\n",
       "\n",
       "\\section{Introduction}\n",
       "\\label{sec:intro}\n",
       "\n",
       "Graph Neural Networks (GNNs)~\\citep{gori2005new,zhou2020graph} have emerged\n",
       "as the standard architecture of choice for modeling atomic systems, with\n",
       "a wide range of applications from protein structure prediction to catalyst discovery and drug design~\\citep{schutt2017quantum,gilmer2017neural,jorgensen2018neural,zitnick2020introduction,schutt2017schnet,xie2018crystal}.\n",
       "These models operate on graph-structured inputs, where nodes of the graph\n",
       "represent atoms, and edges represent bonds or atomic neighbors. Despite their\n",
       "widespread success and the availability of large molecular datasets, training massive GNNs (with up to billions of\n",
       "parameters) is an important but under-explored area. Success of\n",
       "similarly large models in computer vision, natural language processing, and speech\n",
       "recognition~\\citep{shoeybi2020megatronlm,huang2019gpipe,brown2020language,zhai2021scaling}\n",
       "suggests that scaling up GNNs could yield significant performance gains.\n",
       "\n",
       "Most previous approaches to scaling GNNs have focused on scaling small models (with up to a few million parameters) to massive graphs. These methods generally assume that we are working with a large, fixed graph, leading to the development of methods like neighborhood sampling~\\citep{jangda2021accelerating,zheng2021distdgl}\n",
       "%\\lz{or efficient partitioning that depend on the graph structure being provided during training}\n",
       "~\\citep{Jia2020ImprovingTA,Ma2019NeuGraphPD,tripathy2020reducing}.\n",
       "%\n",
       "These methods do not apply to atomic simulation datasets that contain millions of smaller graphs\n",
       "where it is necessary to consider the entire graph for prediction. Our focus is on the complementary problem of scaling to very large models for a dataset of many moderately-sized graphs ($\\sim1k$ nodes, $500k$ edges).\n",
       "\n",
       "Another limitation of existing methods is that they focus on scaling simple GNN architectures such as Graph Convolutional Networks (GCNs) that only represent lower-order interactions~\\textit{i.e.}, representations for nodes and edges, that are then updated by passing messages between neighboring nodes.\n",
       "%\n",
       "In practice, the most successful GNNs used for atomic systems also model higher-order interactions between atoms, such as the interactions between triplets or quadruplets of atoms~\\citep{klicpera_dimenetpp_2020,klicpera2021gemnet,liu2021spherical}.\n",
       "These interactions are necessary to capture the geometry of the underlying system,\n",
       "critical in making accurate predictions. Scaling such GNN architectures is challenging because even moderately-sized graphs can contain a large number of higher-order interactions. For example, a single graph with $1k$ nodes could contain several million triplets of atoms.\n",
       "%\n",
       "In this paper, we introduce \\emph{Graph Parallelism}, an approach to scale up\n",
       "such GNNs with higher-order interactions to billions of parameters,\n",
       "by splitting up the input graph across multiple GPUs.\n",
       "% In this paper, we introduce \\emph{Graph Parallelism}, an approach to split graphs across multiple GPUs, that enables us to scale such GNNs with higher-order interactions towards billions of parameters in model size.\n",
       "\n",
       "We benchmark our approach by scaling up two recent GNN architectures -- DimeNet++~\\citep{klicpera_dimenetpp_2020} and GemNet-T~\\citep{klicpera2021gemnet} -- on the Open Catalyst (OC20) dataset~\\citep{OC20}.\n",
       "%\n",
       "The OC20 dataset, aimed at discovering new catalyst materials for renewable energy storage,\n",
       "consists of $134M$ training examples spanning a wide range of adsorbates and catalyst materials.\n",
       "A GNN that can accurately predict per-atom forces and system energies on OC20\n",
       "has the potential to replace the computationally expensive quantum chemistry calculations based on Density Functional Theory (DFT) that are currently the bottleneck in computational catalysis. Our large-scale, graph-parallelized models lead to relative improvements of\n",
       "1) $15\\%$ for predicting forces on the force MAE metric (S2EF task), and 2) $21\\%$ on the AFbT metric for predicting relaxed structures (IS2RS task), establishing new state-of-the-art results on this dataset.\n",
       "\n",
       "\n",
       "% We present experimental results on the Open Catalyst 2020 (OC20) dataset~\\citep{OC20}. The OC20 dataset was developed for the task of discovering new catalyst materials. This is an important application of GNNs for atomic modeling since discovering new catalysts to drive chemical reactions is essential for addressing energy scarcity, renewable energy storage, and more broadly climate change. Traditionally, density functional theory (DFT) has been used for testing potential catalyst materials by estimating the energy of the atomic system as well as forces exerted on each atom. Unfortunately, the computational complexity of DFT limits its widespread use for testing a large number of materials. The OC20 dataset has been developed for training machine learning approximations of DFT based on GNNs. While existing ML models for approximating DFT have shown some promise, they are not accurate enough to replace DFT. We hypothesize that scaling up these models can yield large improvements in performance.\n",
       "\n",
       "% \\ad{We benchmark Graph Parallelism by scaling up two recent GNN architectures --\n",
       "% DimeNet++~\\citep{klicpera_dimenetpp_2020} and GemNet-dT~\\citep{klicpera2021gemnet} -- on\n",
       "% the Open Catalyst (OC20) dataset~\\citep{OC20}.\n",
       "% The OC20 dataset, aimed at simulating catalyst materials useful for renewable energy storage,\n",
       "% consists of $134M$ DFT relaxation trajectories spanning a wide range of adsorbate\n",
       "% and catalyst chemical compositions.\n",
       "% A GNN that can accurately predict per-atom forces and system energies on OC20\n",
       "% has the potential to significantly speed up or even replace Density Functional Theory (DFT) calculations.\n",
       "% Our large-scale, graph-parallelized models lead to relative improvements of\n",
       "% 1) $15\\%$ on the force MAE metric on the S2EF task and 2) $21\\%$ on the AFbT\n",
       "% metric on the IS2RS task, establishing new state-of-the-art results on OC20.}\n",
       "\n",
       "% In this paper, we describe the Graph Parallelism paradigm and demonstrate how to scale up two existing GNN models, DimeNet++ \\cite{} and GemNet-T \\cite{}. We empirically demonstrate the benefits of scaling up these models on the Open Catalyst 2020 (OC20) dataset aimed at simulating catalyst materials that are useful for climate change applications. These models obtain state-of-the-art results on this dataset, beating the previous SOTA by XX\\% and YY\\% respectively.\n",
       "% TODO Further claims depending on the results.\n",
       "\n",
       "% \\TODO{AD: Yet to read the approach section in detail, but we should probably add\n",
       "% a sentence or two here in the intro about energy-centric vs. force-centric force estimation as well.}\n",
       "\n",
       "\\section{Graph Parallelism}\n",
       "\\label{sec:graphparallel}\n",
       "\n",
       "\\subsection{Extended Graph Nets}\n",
       "\\label{sec:egn}\n",
       "\n",
       "\\cite{battaglia2018relational} introduced a framework called Graph Network (GN) that provides a general abstraction for many popular Graph Neural Networks (GNNs) operating on edge and node representations of graphs. We build on their work and define the Extended Graph Network (EGN) framework to include GNNs that also operate on higher order terms like triplets or quadruplets of nodes.\n",
       "\n",
       "In the Graph Network (GN) framework, a graph is defined as a $3$-tuple $\\GGG = (\\uu, V, E)$, where $\\uu$ represents global attributes about the entire graph; $V = \\{\\vv_i\\}_{i=1:N^v}$ is the set of all nodes, with $\\vv_i$ representing the attributes of node $i$; and $E = \\{(\\ee_k, r_k, s_k)\\}_{k=1:N^e}$ is the set of all edges where $\\ee_k$ represents the attributes for the edge from node $s_k$ to $r_k$. A GNN then contains a series of GN blocks\n",
       "that iteratively operate on the input graph, updating the various representations.\n",
       "\n",
       "In our \\emph{Extended Graph Network (EGN)} framework, a graph is defined as a $4$-tuple $\\GGG = (\\uu, V, E, T)$, where $\\uu, V,$ and $E$ are defined as in the Graph Network, and $T = \\{(\\ttt_m, e_{m_1}, e_{m_2}, \\ldots)\\}$ is the set of higher-order interaction terms involving edges indexed by $m_1, m_2, \\ldots$.\n",
       "\n",
       "As a concrete example, consider an atomic system represented as a graph in this framework with the nodes representing the atoms and the edges representing atomic neighbors. The node attributes $v_i$ and edge attributes $e_k$ could represent the atom's atomic numbers and distances between atoms respectively. The higher order interactions could represent triplets of atoms, \\textit{i.e.}, pairs of neighboring edges with $\\ttt_m$ representing the bond angle, which is the angle between edges that share a common node.\n",
       "Finally, the global attribute $\\uu$\n",
       "can represent the energy of the system. For clarity of exposition, we will limit our discussion to triplets in the rest of the paper, but higher order interactions can be handled in a similar manner. We denote these triplets as $(\\ttt_m, e_{m_1}, e_{m_2})$.\n",
       "\n",
       "In the EGN framework, GNNs then contain a series of \\emph{EGN blocks} that iteratively update the graph. Each EGN block consists of several \\emph{update} and \\emph{aggregation} functions that are applied to transform an input graph $(\\uu, V, E, T)$ into an output graph $(\\uu', V', E', T')$. %We call these functions $\\TU, \\TA, \\EU $ etc.\n",
       "Each EGN block starts by updating the highest order interactions, which are then aggregated before updating the next highest order interaction. For instance, the triplet representations are first updated (using $\\TU$ function) and then aggregated ($\\TA$) at each edge. These aggregated representations are then used to update the edge representation ($\\EU$). Next, the edges going into a node are aggregated and used to update the node representations, and so on. This is illustrated in figure \\ref{fig:egn_block}.\n",
       "\n",
       "% \\begin{enumerate}\n",
       "%     \\item Update each triplet representation:\n",
       "%     \\begin{equation} \\label{eq:triplet_update}\n",
       "%         \\ttt'_m = \\TU (\\ttt_m, \\ee_{m_1}, \\ee_{m_2}, \\uu)\n",
       "%     \\end{equation}\n",
       "%     \\item For each edge $k$, aggregate the updated representations of all triplets involving edge $k$:\n",
       "%     \\begin{equation} \\label{eq:triplet_aggr}\n",
       "%         \\Bar{\\ttt}'_k = \\TA (T'_k)\n",
       "%     \\end{equation}\n",
       "%     where $T'_k = \\{ (\\ttt'_m, e_{m_1}, e_{m_2}) | e_{m_1} = k \\}$.\n",
       "\n",
       "%     \\item Update each edge representation using the aggregated triplet representations:\n",
       "%     \\begin{equation} \\label{eq:edge_update}\n",
       "%         \\ee'_k = \\EU (\\ee_k, \\vv_{r_k}, \\vv_{s_k}, \\Bar{\\ttt}'_k, \\uu)\n",
       "%     \\end{equation}\n",
       "%     \\item For each node $i$, aggregate the updated edge representations for all edges incident on node $i$:\n",
       "%     \\begin{equation} \\label{eq:edge_aggr}\n",
       "%         \\Bar{\\ee}'_i = \\EA (E'_i)\n",
       "%     \\end{equation}\n",
       "%     where $E'_i = \\{ (\\ee'_k, r_k, s_k) | r_k = i \\}$.\n",
       "\n",
       "%     \\item Update each node representation using the aggregated edge representations:\n",
       "%     \\begin{equation} \\label{eq:node_update}\n",
       "%         \\vv'_i = \\NU (\\vv_i, \\Bar{\\ee}'_i, \\uu)\n",
       "%     \\end{equation}\n",
       "\n",
       "%     \\item Finally, aggregate information from all the nodes and update the global attributes:\n",
       "%     \\begin{equation} \\label{eq:node_update2}\n",
       "%       \\Bar{\\vv}' = \\NA (V')\n",
       "%     \\end{equation}\n",
       "%     \\begin{equation} \\label{eq:node_aggr}\n",
       "%       \\uu' = \\GU (\\Bar{\\vv}', \\uu)\n",
       "%     \\end{equation}\n",
       "%     where $V' = \\{\\vv'_i\\}_{i=1:N^v}$\n",
       "\n",
       "% \\end{enumerate}\n",
       "\n",
       "Many GNNs can be cast in the EGN framework using appropriate update and aggregation functions.\n",
       "\n",
       "% \\begin{figure}\n",
       "%     \\centering\n",
       "%     \\includegraphics[width=0.8\\linewidth]{iclr2022/extended_graph_nets.pdf}\n",
       "%     \\caption{Forward computation of an EGN block. First, each triplet representation is updated using the $\\TU$ function, followed by an aggregation of these updated representations ($\\TA$). These aggregated values are then used to update the edge representations ($\\EU$), followed by an edge aggregation ($EA$). This process is continued in a similar manner to update the node and global representations as well.}\n",
       "%     \\label{fig:egn_block}\n",
       "% \\end{figure}\n",
       "\n",
       "% \\begin{figure}\n",
       "%     \\centering\n",
       "%     \\includegraphics[width=0.8\\linewidth]{iclr2022/graph_parallel.pdf}\n",
       "%     \\caption{Distributed computation of an EGN block using graph parallelism. The graph is split up among the different GPUs such that processing unit $p$ contains its subset of triplets $T^{(p)}$ in memory, along with the entire set of edges $E$ and nodes $V$. The triplet attributes are updated first, followed by a triplet aggregation to update the edge attributes locally. Next, an allreduce operation is performed to update all edge attributes globally. We repeat this process to update the node and global attributes.}\n",
       "%     \\label{fig:graph_parallel}\n",
       "% \\end{figure}\n",
       "\n",
       "\\begin{figure}[h]\n",
       "\\centering\n",
       "    \\begin{subfigure}[b]{0.9\\textwidth}\n",
       "        \\centering\n",
       "        \\includegraphics[width=\\textwidth]{extended_graph_nets.pdf}\n",
       "        \\caption{Forward computation of an EGN block. First, each triplet representation is updated ($\\TU$ function),\n",
       "        followed by aggregation of these updated representations ($\\TA$).\n",
       "        Next, these aggregated values are used to update edge representations ($\\EU$), followed by edge aggregation ($\\EA$),\n",
       "        and finally node and global updates.}\n",
       "        \\label{fig:egn_block}\n",
       "    \\end{subfigure}\n",
       "\n",
       "    \\vspace{0.2cm}\n",
       "    \\begin{subfigure}[b]{0.9\\textwidth}\n",
       "    \\centering\n",
       "    \\includegraphics[width=\\textwidth]{graph_parallel.pdf}\n",
       "    \\caption{Distributed computation of an EGN block. The graph is split up among the different GPUs such that processing unit $p$ contains its subset of triplets $T^{(p)}$ in memory, along with all edges $E$ and nodes $V$. The triplet attributes are updated in parallel, followed by a triplet aggregation to locally update the edge attributes. Next, an {\\tt allreduce} operation is performed to update all edge attributes globally. We continue this process to update the node and global attributes.}\n",
       "    \\label{fig:graph_parallel}\n",
       "\n",
       "    \\end{subfigure}\n",
       "\n",
       "    \\label{Sequential and distributed computation of an EGN block.}\n",
       "\\end{figure}\n",
       "\n",
       "\\subsection{Graph Parallelism for Extended Graph Nets}\n",
       "\\label{sec:gp-egn}\n",
       "\n",
       "Training large EGNs can be challenging even on moderately sized graphs because of the large memory footprint required in storing and updating the representation for each triplet, edge, and node. In many applications, the number of edges is one or two orders of magnitude larger than the number of nodes, while the number of triplets is one or two orders of magnitude larger than the number of edges. Thus, storing and updating the triplet representations is often the bottleneck in terms of GPU memory and compute. Many recent methods such as \\citep{klicpera_dimenetpp_2020,klicpera2021gemnet} overcome this problem by using a very low-dimensional representation for the triplets. However, we found this to significantly reduce model capacity leading to underfitting for some applications with large training datasets. It is necessary to overcome these memory limitations for better generalization.\n",
       "\n",
       "One way to avoid the memory limits is to distribute the computation across multiple GPUs. For a graph neural network, a natural choice to distribute the computation is by splitting the graph.\n",
       "The update functions in an EGN are easy to apply in parallel since they are applied independently for each triplet, edge, or node. It is substantially more challenging to apply the aggregations in parallel. To simplify parallel aggregation, we restrict ourselves to aggregation functions that are commutative and associative. This is not limiting in practice since most popular GNN architectures use sum or mean aggregations which can be implemented in this manner.\n",
       "\n",
       "We will now describe the implementation of the distributed EGN block. Suppose we have access to $P$ processing units that we wish to split the graph computation over. Each unit $p$ is responsible for computing the updates to a subset of triplets, edges and nodes, that we denote by $T^{(p)}, E^{(p)},$ and $V^{(p)}$, respectively. At the beginning of computation, we split the graph so that the processing unit $p$ contains its subset of triplets $T^{(p)}$ in memory, along with the entire set of edges $E$ and nodes $V$. During the forward pass, we first update each set of triplets $T^{(p)}$ in parallel to obtain $T'^{(p)}$, followed by a local triplet aggregation. Next, an all-reduce operation is performed on these locally aggregated triplet representations to obtain globally aggregated triplet representations. These globally aggregated representations are then used to update edges in parallel, and the process is repeated for nodes and ultimately for the global graph level representation.\n",
       "% \\bw{the second half of previous sentence is a bit confusing. If we want to follow the EGN framework should say something like the following} \\bw{During the forward pass, we first update each set of triplets $T^{(p)}$ in parallel, followed by local triplet aggregation. Next, an all-reduce operation is performed to aggregate these locally updated triplet representations into globally updated triplet representations, $T'$. The updated triplet representations are then used to update edges ($E'$) and the process is repeated for nodes ($V'$) and ultimately for the global graph level update ($\\uu'$). This process is illustrated ...}  Next, an all-reduce operation is performed to aggregate these locally updated edge representations into globally updated edge representations, $E'$. Next, we repeat the same process with edges to update the node representations, and with the node representations to update the global features.\n",
       "Figure~\\ref{fig:graph_parallel} shows this.\n",
       "\n",
       "% Computation then proceeds as follows:\n",
       "%\\lz{Okay it makes sense know why you wrote out all the steps above. It might be better to put both of these side by side in a figure? I.e., one for serial graph computation and one for parallel. This will also save space probably.}\n",
       "\n",
       "% \\begin{enumerate}\n",
       "%     \\item In each processor $p$ in parallel, update the representations for each triplet in $T^{(p)}$ in parallel using equation \\ref{eq:triplet_update}.\n",
       "%     \\item For each edge $k$, in each processor $p$, locally aggregate the triplet representations, followed by an all-reduce for global aggregation.\n",
       "%     \\begin{equation} \\label{eq:triplet_aggr2}\n",
       "%     \\begin{split}\n",
       "%         \\Bar{\\ttt}'^{(p)}_k &= \\TA (T'^{(p)}_k) \\\\\n",
       "%         \\Bar{\\ttt}'_k &= \\AllReduce ( \\{ \\Bar{\\ttt}'^{(p)}_k \\}_{p=1:P} )\n",
       "%     \\end{split}\n",
       "%     \\end{equation}\n",
       "%     At this stage, all processors have the aggregated representations $\\Bar{\\ttt}'_k$.\n",
       "\n",
       "%     \\item In each processor $p$ in parallel, update the representations for each edge in $E^{(p)}$ in parallel using equation \\ref{eq:edge_update}.\n",
       "%     \\item For each node $i$, in each processor $p$, locally aggregate the edge representations, and then perform an all-reduce step to globally aggregate the edge representations.\n",
       "\n",
       "%     \\begin{equation} \\label{eq:edge_aggr2}\n",
       "%     \\begin{split}\n",
       "%         \\Bar{\\ee}'^{(p)}_i &= \\EA (E'^{(p)}_i) \\\\\n",
       "%         \\Bar{\\ee}'_i &= \\AllReduce ( \\{ \\Bar{\\ee}'^{(p)}_i \\}_{p=1:P} )\n",
       "%     \\end{split}\n",
       "%     \\end{equation}\n",
       "\n",
       "%     \\item In each processor $p$ in parallel, update the representations for each node in $V^{(p)}$ using equation \\ref{eq:node_update}.\n",
       "\n",
       "%     \\item Finally, locally aggregate node representations, all-reduce the results and update the global attributes.\n",
       "%     \\begin{equation} \\label{eq:node_update3}\n",
       "%     \\begin{split}\n",
       "%       \\Bar{\\vv}'^{(p)} &= \\NA (V'^{(p)}) \\\\\n",
       "%       \\Bar{\\vv}' &= \\AllReduce ( \\{ \\Bar{\\vv}'^{(p)}_i \\}_{p=1:P} ) \\\\\n",
       "%       \\uu' &= \\GU (\\Bar{\\vv}', \\uu)\n",
       "%     \\end{split}\n",
       "%     \\end{equation}\n",
       "%     where $V' = \\{\\vv'_i\\}_{i=1:N^v}$.\n",
       "\n",
       "% \\end{enumerate}\n",
       "\n",
       "In this framework, the highest order interaction attributes are never communicated across processors. Therefore, the communication time is bound by the number of lower order node interactions. In our example, the triplet representations are not communicated, while the edge and node representations are communicated once per EGN block, making the total communication cost equal to $O(N_v D_v + N_e D_e)$, where $D_v$ and $D_e$ are the dimensions of node and edge representations. This makes it possible to work with a large number of triplets and large triplet representations. In section \\ref{sec:gp_atoms}, we show concrete instantiations of this framework for two contemporary GNNs.\n",
       "\n",
       "% triplet representations are never communicated across processors. This makes it possible to work with large triplet representations without running out of memory. The edge representations and node representations are communicated once per EGN block, making the total communication cost equal to $O(N_v N_e D_v D_e)$ where $D_v$ and $D_e$ are the dimensions of node and edge representations.\n",
       "\n",
       "% \\lz{If you did use this for interactions higher order than triplets, it might not work since you have to enumerate at triplets per GPU correct? If this isn't the case, I'd call it out.}\n",
       "% \\as{Actually, I have second thoughts about adding this. Given our definition of EGN, we only have one set of higher order interactions that need to be updated before updating edges. With this definition, the approach works (and works for GemNet-Q as well). If we had a model that uses the quadruplets to update the triplets, then triplets to edges, and so on, that would change the definition of the EGN to a 6-tuple (u, V, E, T, Q).}\n",
       "% Since this approach only communicates node and edge representation, and avoids communicating triplet representations, the size of the graphs that we can apply this method to is limited by the number of edges. If we apply this to an EGN with even higher order interactions, such as quadruplets of atoms, that need to be updated before triplets are updated, then the size of the graphs will be limited by the number of triplets\n",
       "\n",
       "\\section{Graph Parallelism for Atomic Simulations} \\label{sec:gp_atoms}\n",
       "\n",
       "In this section, we present two concrete examples of using GNNs for the problem of predicting the energy and the forces for an atomic system, modeled as a graph whose nodes represent the atoms and whose edges represent the atoms' neighbors. The GNN takes such a graph as input and predicts the energy of the entire system as well as a 3D force vector on each atom. Two paradigms have been proposed in the literature, that we call \\emph{energy-centric} and \\emph{force-centric} approaches. These approaches differ in how they estimate forces and whether they are energy conserving, which is an important physical property. We begin by describing the components shared by both approaches, followed by one recent model in each paradigm.\n",
       "\n",
       "\\subsection{Inputs and Outputs}\n",
       "\n",
       "The inputs to the network are 3D positions $\\xx_i \\in \\R^3$ and atomic number $z_i$ for each atom $i \\in \\{1\\ldots n\\}$.\n",
       "The outputs are the per-atom forces $\\ff_i \\in \\R^3$ and the energy $E \\in \\R$ of the entire structure. The distance between atoms $i$ and $j$ is denoted as $d_{ij} = || \\xx_i - \\xx_j ||$. If edges $(i, j)$ and $(j, k)$ exist, then $(i, j, k)$ defines a \\emph{triplet} in the graph and we denote the angle between the edges as $\\alpha_{kj,ji}$.\n",
       "\n",
       "The input graph is constructed with each atom $t$ (\\emph{target}) as a node and the edges representing the atom's neighbors $s \\in N_t$ where $N_t$ contains all atoms $s$ (\\emph{source}) within a distance $\\delta$, which is treated as a hyperparameter. Each edge has a corresponding message $m_{st}$ that passes information from source atom $s$ to target atom $t$.\n",
       "\n",
       "\\subsection{Estimating Forces and Energy}\n",
       "\n",
       "As previously stated, there are two paradigms for estimating the energy and forces for an atomic system: \\emph{energy-centric} and \\emph{force-centric}. In energy-centric models, the model first computes the energy $E$ by applying a forward pass of the GNN: $E = \\GG(\\xx, \\zz)$, where $\\xx$, and $\\zz$ represent the atomic positions and atomic numbers respectively. The forces are then computed as the negative gradient of the energy with respect to atomic positions by using backpropagation: $\\ff = -\\nabla_{\\xx} E$.\n",
       "\n",
       "In force-centric models, the energy and forces are both computed directly during the forward propagation: $E, \\FF = \\GG(\\xx, \\zz)$ where $\\FF$ represents the matrix of all atomic forces. Force-centric models tend to be more efficient in terms of computation time and memory usage compared to the energy-centric models. However, energy-centric models guarantee that the forces are energy conserving which is an important physical property satisfied by atomic systems. In this work, we demonstrate the benefits of scaling up GNNs in both paradigms.\n",
       "\n",
       "\\subsection{Energy-Centric model: DimeNet++}\n",
       "\n",
       "DimeNet++~\\citep{klicpera_dimenetpp_2020} is a recently proposed energy-centric model for atomic systems. In this model, the edges are represented by a feature representation $\\mm_{ji}$, which are iteratively updated using both directional information (via bond angles) as well as the interatomic distances. The edges are initially represented using a radial basis function (RBF) representation $\\ee^{(ji)}_{RBF}$ of their lengths $d_{ji}$. The triplets are represented using a spherical basis function (SBF) expansion $\\aaa^{(kj,ji)}_{SBF}$ of the distances $d_{kj}$ as well as bond angles $\\alpha_{(kj,ji)}$. In each block, the messages are updated as:\n",
       "%\n",
       "\\begin{equation}\n",
       "    \\mm_{ji}' = \\Fupdate (\\mm_{ji}, \\sum_{k\\in \\N_j \\textbackslash \\{i\\}} \\Fint(\\mm_{kj}, \\ee^{(ji)}_{RBF}, \\aaa^{(kj,ji)}_{SBF}))\n",
       "\\end{equation}\n",
       "%\n",
       "where the interaction function $\\F_{int}$ corresponds to the $\\TU$ function, the summation corresponds to the $\\TA$ function, and the update function $\\F_{update}$ to the $\\EU$ function respectively. The interaction function consists of a Hadamard product of the embeddings, followed by a multi-layer perceptron. To speed up these computations, the embeddings are projected down to a smaller dimension before computing interactions, and later projected back up.\n",
       "\n",
       "The updated messages are then fed as input to an output block that sums them up per atom $i$ to obtain a per-atom representation: $\\hh_i = \\sum_j \\mm'_{ji}$ ($\\EA$ function). These are then transformed by another MLP to obtain the node representation $\\vv'_i$ ($\\NU$ function).\n",
       "\n",
       "Thus, DimeNet++ is a case of EGN, and we closely follow the recipe from Sec.~\\ref{sec:gp-egn} to parallelize it.\n",
       "\n",
       "\\subsection{Force-Centric model: GemNet}\n",
       "\n",
       "GemNet~\\citep{klicpera2021gemnet} extends DimeNet++ in a number of ways, including the addition of quadruplets as well as a force-centric version. Here, we focus only on the force-centric GemNet-T model since it was recently shown to obtain state-of-the-art results on the OC20 dataset\\footnote{{\\href{https://opencatalystproject.org/leaderboard.html}{\\tt opencatalystproject.org/leaderboard.html}}}. GemNet-T largely follows the structure of the DimeNet++ model, but includes some modifications.\n",
       "\n",
       "First, GemNet-T uses a bilinear layer instead of the Hadamard product for the interaction function. This is made efficient by optimally choosing the order of operations in the bilinear function to minimize computation. Second, GemNet-T maintains an explicit embedding for each atom that is first updated by aggregating the directional embeddings involving that atom, similar to DimeNet++. Next, the updated atom embedding is used to update each of the edge embeddings. This creates a second edge update function, $\\EU'$, that is run after the node embeddings are updated. Third, GemNet makes use of symmetric message passing, that is the messages $\\mm_{ji}$ and $\\mm_{ij}$ that are on the same edge, but in different directions, are coupled.\n",
       "In a parallel implementation, this step requires an additional all-reduce step since messages $\\mm_{ji}$ and $\\mm_{ij}$ could be on different processors.\n",
       "\n",
       "Thus, GemNet-T largely follows the EGN framework, with a few minor deviations from the standard formulation. The distributed EGN implementation described in section \\ref{sec:gp-egn} can be used for the GemNet-T as well, but with additional communication steps to account for the second edge update function and symmetric message passing.\n",
       "\n",
       "\\section{Experiments}\n",
       "% setup -- OCP dataset, compute infra, experiment parameters\n",
       "\n",
       "In this section, we present the results of our scaling experiments on the Open Catalyst 2020 (OC20) dataset~\\citep{OC20}. The OC20 dataset contains over 130 million atomic structures used to train models for predicting forces and energies during structure relaxations. We report results for three tasks: 1) Structure to Energy and Forces (S2EF) that involves predicting energy and forces for a given structure; 2) Initial Structure to Relaxed Energy (IS2RE) that involves predicting the relaxed energy for a given initial structure; and 3) Initial Structure to Relaxed Structure (IS2RS) which involves performing a structure relaxation using the predicted forces. DimeNet++ and GemNet-T are the current state-of-the-art energy-centric and force-centric models respectively.\n",
       "\n",
       "\\subsection{Experimental Setup}\n",
       "\n",
       "\\textbf{DimeNet++-XL}. Our DimeNet++ model consists of $B=4$ interaction blocks, with a hidden dimension of $H = 2048$, an output block dimension of $D = 1536$, and intermediate triplet dimension of $T = 256$. This model has about $240M$ parameters, which is over $20\\times$ larger than the DimeNet++-large model used in~\\citep{OC20}. We call this model \\emph{DimeNet++-XL}. The model was trained with the AdamW optimizer~\\citep{kingma2014adam,loshchilov2019decoupled} starting with an initial learning rate of $10^{-4}$, that was multiplied by $0.8$ whenever the validation error plateaus. The model was trained with an effective batch size of 128 on 256 Volta 32GB GPUs with a combination of data parallel and graph parallel training: each graph was split over 4 GPUs with data parallel training across groups of 4 GPUs.\n",
       "\n",
       "\\textbf{GemNet-XL}. Our GemNet model consists of $B = 6$ interaction blocks, with an edge embedding size of $E=1536$, triplet embedding size of $T=384$ and embedding dimension of the bilinear layer of $B=192$. We found that it was beneficial to use a small atom embedding size of $A=128$, much smaller than the previous SOTA GemNet model\\footnote{\\href{https://discuss.opencatalystproject.org/t/new-gemnet-dt-code-results-model-weights/102}{\\tt discuss.opencatalystproject.org/t/new-gemnet-dt-code-results-model-weights/102}}. This model has roughly 300M parameters, which is about $10\\times$ larger than the previous SOTA model. However, since we reduced the atom embedding dimension and increased the edge and triplet dimensions, the total amount of compute and memory usage is significantly larger. We call this model \\emph{GemNet-XL}. We followed the same training procedure as with DimeNet++, except for a starting learning rate of $2\\times10^{-4}$.\n",
       "\n",
       "% TODO: Describe Loss function, cutoff radius etc.\n",
       "\n",
       "\\subsection{Structure to Energy and Forces (S2EF)}\n",
       "The Structure to Energy and Forces task takes an atomic structure as input and predicts the energy of the entire structure and per-atom forces. The S2EF task has four metrics: the energy and force Mean Absolute Error (MAE), the Force Cosine Similarity, and the Energy and Forces within a Threshold (EFwT). EFwT indicates the percentage of energy and force predictions below a preset threshold.\n",
       "\n",
       "Table \\ref{tab:s2ef_results} compares the top models on the Open Catalyst Project leaderboard$^1$ with the GemNet-XL model. GemNet-XL obtains a roughly $16\\%$ lower force MAE and an $8\\%$ lower energy MAE relative to the previous state-of-the-art. Further, GemNet-XL improves the EFwT metric more than $50\\%$ relative to the previous best, although the value is still very small. These results indicate that model scaling is beneficial for the S2EF task.\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{5pt}\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{lrrrrrr}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{\\#Params}} & \\mr{2}{\\textbf{Training GPU-days}} & \\mc{4}{c}{\\textbf{S2EF Test}}\\\\\n",
       "        & & & Energy MAE (eV)$\\downarrow$ & Force MAE (eV/A)$\\downarrow$ & Force Cos$\\uparrow$ & EFwT$\\uparrow$\\\\\n",
       "        \\midrule\n",
       "        ForceNet-Large~\\citep{hu2021forcenet} & 34.8M & 194 & 2.2628\t &0.03115\t& 0.5195\t& 0.01\\% \\\\\n",
       "        DimeNet++-Large~\\citep{klicpera_dimenetpp_2020} & 10.8M & 1600 & 31.5409\t& 0.03132\t& 0.5440\t& 0.00\\% \\\\\n",
       "        SpinConv~\\citep{shuaibi_rotation_2021} & 8.9M & 76 & 0.3363 &\t0.02966 &\t0.5391 &\t0.45\\%\t\\\\\n",
       "        GemNet-T~\\citep{klicpera2021gemnet} & 31M & 47 & 0.2924 & 0.02422 &\t0.6162\t& 1.20\\% \\\\\n",
       "        \\midrule\n",
       "        % DimeNet++-XL (force) & 240M & XX & - & XX & XX & XX \\\\\n",
       "        GemNet-XL & 300M & 1962 & \\textbf{0.2701} & \\textbf{0.02040} & \\textbf{0.6603} & \\textbf{1.81\\%} \\\\\n",
       "        \\bottomrule\n",
       "         % TODO: Add #params.\n",
       "    \\end{tabular}}\n",
       "    \\caption{Experimental results on the S2EF task comparing our GemNet-XL to the top entries on the Open Catalyst leaderboard, showing metrics averaged across the 4 test datasets.}\n",
       "    \\label{tab:s2ef_results}\n",
       "    \\vspace{-5pt}\n",
       "\\end{table*}\n",
       "\n",
       "\\subsection{Initial Structure to Relaxed Structure (IS2RS)}\n",
       "The Initial Structure to Relaxed Structure (IS2RS) task involves taking an initial atomic structure and predicting the atomic structure that minimizes the energy. This is performed by iteratively predicting the forces on each atom and then using these forces to update the atomic positions. This process is repeated until convergence or 200 iterations. There are three metrics for this task: Average Distance within Threshold (ADwT), that measures the fraction of final atomic positions within a distance threshold of the ground truth; Forces below Threshold (FbT), which measures whether a true energy minimum was found (\\textit{i.e.}, forces are smaller than a preset threshold); and, the Average Forces below Threshold (AFbt), which averages the FbT over several thresholds.\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.8\\linewidth}{!}{\n",
       "    \\small\n",
       "    \\begin{tabular}{lrcrrr}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{\\#Params}} & \\textbf{Training} & \\mc{3}{c}{\\textbf{IS2RS Test}} \\\\\n",
       "        & & \\textbf{Dataset} & AFbT$\\uparrow$ & ADwT$\\uparrow$ & FbT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        SpinConv~\\citep{shuaibi_rotation_2021} & 8.9M & S2EF-All & 16.67\\% &\t53.62\\% &\t0.05\\% \\\\\n",
       "        DimeNet++~\\citep{klicpera_dimenetpp_2020} & 1.8M & S2EF 20M + MD\t& 17.15\\%\t& 47.72\\%\t& 0.15\\% \\\\\n",
       "        DimeNet++-large~\\citep{klicpera_dimenetpp_2020} & 10.8M & S2EF-All & 21.82\\% & 51.68\\% & 0.40\\% \\\\\n",
       "        GemNet-T~\\citep{klicpera2021gemnet} & 31M & S2EF-All &\t27.60\\%\t& 58.68\\%\t& 0.70\\% \\\\\n",
       "        \\midrule\n",
       "        DimeNet++-XL & 240M & S2EF 20M + MD & \\textbf{33.44\\%} & 59.21\\% & \\textbf{1.25\\%} \\\\\n",
       "        GemNet-XL & 300M & S2EF-All & 30.82\\% & \\textbf{62.65\\%} & 0.90\\%\\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Results on the IS2RS task comparing our models to the top entries on the Open Catalyst leaderboard, showing metrics averaged across the 4 test datasets. The DimeNet++ and DimeNet++-XL models were trained on the S2EF 20M + MD dataset, that contains additional molecular dynamics data and has been shown to be helpful for the IS2RS task~\\citep{OC20}.}\n",
       "    \\label{tab:is2rs_results}\n",
       "    \\vspace{-15pt}\n",
       "\\end{table*}\n",
       "\n",
       "\n",
       "\\begin{table*}[h]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.6\\linewidth}{!}{\n",
       "    \\small\n",
       "    \\begin{tabular}{llrr}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{Approach}} & \\mc{2}{c}{\\textbf{IS2RE Test}}\\\\\n",
       "        & & Energy MAE (EV)$\\downarrow$ & EwT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        SpinConv~\\citep{shuaibi_rotation_2021} & Relaxation & 0.4343\t& 7.90\\% \\\\\n",
       "        GemNet-T~\\citep{klicpera2021gemnet} & Relaxation & 0.3997\t& 9.86\\% \\\\\n",
       "        Noisy Nodes~\\citep{godwin_iclr22} & Direct\t& 0.4728\t& 6.50\\%\t\\\\\n",
       "        3D-Graphormer~\\citep{graphormer} & Direct\t& 0.4722\t& 6.10\\%\t\\\\\n",
       "        \\midrule\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3712} & \\textbf{11.13\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4623 & 5.60\\% \\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Results on the IS2RE task comparing our GemNet-XL to the top entries on the Open Catalyst leaderboard, showing metrics averaged across the 4 test datasets.}\n",
       "    \\label{tab:is2re_results}\n",
       "\\end{table*}\n",
       "\n",
       "Table \\ref{tab:is2rs_results} shows the results on the IS2RS task, comparing our models with the top few models on the Open Catalyst leaderboard. Both the DimeNet++-XL and GemNet-XL models outperform all existing models. The DimeNet++-XL model obtains a relative improvement of 53\\%  on the AFbT metric, and more than triples the FbT metric compared to the DimeNet-large model. The GemNet-XL model obtains similar improvements compared to the smaller GemNet-T model. These results underscore the importance of model scaling for this task.\n",
       "\n",
       "\\subsection{Initial Structure to Relaxed Energy (IS2RE)}\n",
       "\n",
       "The Initial Structure to Relaxed Energy (IS2RE) task takes an initial atomic structure and attempts to predict the energy of the structure after it has been relaxed. Two approaches can be taken to address this problem, the direct and the relaxation approaches~\\citep{OC20}. In the direct approach, we treat this task as a regression problem, and train a model to directly estimate the relaxed energy for a given atomic structure. The relaxation approach first estimates the relaxed structure (IS2RS task) after which the energy is estimated using the relaxed structure as input. Relaxation approaches typically outperform the direct approaches, though they are generally two orders of magnitude slower during inference time due to the need to estimate the relaxed structure.\n",
       "\n",
       "Table \\ref{tab:is2re_results} compares our GemNet-XL model to the top three models from the Open Catalyst Project leaderboard. There are two metrics for the IS2RE task: energy Mean Absolute Error (MAE) and the Energy within Threshold (EwT) which measures the percentage of time the predicted energy is within a threshold of the true energy. Table \\ref{tab:is2re_results} shows that the GemNet-XL model obtains a roughly 8\\% lower energy MAE and a 12\\% higher EwT compared to the previous best, which is the smaller GemNet-T model, demonstrating the benefits of scaling up IS2RE models.\n",
       "\n",
       "Since direct IS2RE models are very fast at inference time, there are applications where they are more useful than relaxation based approaches.\n",
       "It is possible to convert a trained S2EF model into a direct IS2RE model by fine-tuning it on the IS2RE training data. We finetune our GemNet-XL model in this manner for 5 epochs, starting with an initial learning rate of $3\\times10^{-5}$ that is exponentially decayed by multiplying with $0.95$ at the end of each epoch. The resuting model -- GemNet-XL-FT -- obtains a relative improvement of ${\\sim}2\\%$ on energy MAE compared to\n",
       "3D-Graphormer~\\citep{graphormer}, the current state-of-the-art direct approach (Table \\ref{tab:is2re_results}).\n",
       "\n",
       "% \\begin{table*}[t]\n",
       "%     \\centering\n",
       "%     \\renewcommand{\\arraystretch}{1.0}\n",
       "%     \\setlength{\\tabcolsep}{6pt}\n",
       "%     % \\resizebox{0.97\\linewidth}{!}{\n",
       "%     \\begin{tabular}{c|c|c|c|c|c}\n",
       "%         \\toprule\n",
       "%         \\textbf{Edge Dim} & \\textbf{Atom Dim} & \\textbf{Triplet Dim} & \\textbf{Bilinear Dim} & \\textbf{Num Params} & \\textbf{Num GPUs}\\\\\n",
       "%         \\midrule\n",
       "%         1024 & 1024 & 128 & 64 & 125.4M & 1 \\\\\n",
       "%         1504 & 1504 & 160 & 96 & 269.9M & 2 \\\\\n",
       "%         2048 & 2048 & 256 & 128 & 500.5M & 4 \\\\\n",
       "%         3072 & 3072 & 384 & 192 & 1.12B & 8 \\\\\n",
       "%         \\bottomrule\n",
       "%     \\end{tabular}\n",
       "%     % }\n",
       "%     \\caption{Model hyperparameters for the scaling analysis. Number of interaction blocks is kept fixed at 3.\n",
       "%     For weak scaling efficiency by batch size, we only use the 125.4M parameter model, and for\n",
       "%     weak scaling efficiency by model size, we train all 4 model sizes on different number of GPUs.}\n",
       "%     \\label{tab:scaling_models}\n",
       "%     \\vspace{-14pt}\n",
       "% \\end{table*}\n",
       "\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{c|c|c|c|c|c|c|c}\n",
       "        \\toprule\n",
       "        \\textbf{\\#Blocks} & \\textbf{Node Dim} & \\textbf{Edge Dim} & \\textbf{Trip Dim} & \\textbf{Bil Dim} & \\textbf{Params} & \\textbf{\\#GP GPUs} & \\textbf{\\#GP+DP GPUs}\\\\\n",
       "        \\midrule\n",
       "        3 & 1280 & 768 & 128 & 64 & 125M & 1 & 32 \\\\\n",
       "        4 & 1536 & 1024 & 192 & 96 & 245M & 2 & 64 \\\\\n",
       "        6 & 1792 & 1184 & 288 & 160 & 480M & 4 & 128 \\\\\n",
       "        8 & 2320 & 1302 & 512 & 288 & 960M & 8 & 256 \\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    }\n",
       "    \\caption{Model hyperparameters for the scaling analysis. ``\\#GP GPUs'' denotes the number of GPUs over which the graph is distributed over for pure graph parallel training on a single node. ``\\#GP+DP GPUs'' denotes the total number of GPUs used to train with graph parallel training together with 32-way data parallel training.}\n",
       "    \\label{tab:scaling_models}\n",
       "    \\vspace{-14pt}\n",
       "\\end{table*}\n",
       "\n",
       "\n",
       "% \\begin{figure}[h]\n",
       "%      \\begin{subfigure}[b]{0.30\\textwidth}\n",
       "%          \\centering\n",
       "%          \\includegraphics[scale=0.2]{Weak Scaling (batch size).pdf}\n",
       "%          \\caption{Weak scaling efficiency by batch size. The model size is fixed and batch size is set proportional to the number of GPUs.}\n",
       "%          \\label{fig:scaling_batch}\n",
       "%      \\end{subfigure}\n",
       "%      \\quad\n",
       "%      \\begin{subfigure}[b]{0.34\\textwidth}\n",
       "%          \\centering\n",
       "%          \\includegraphics[scale=0.22]{Weak Scaling (model size).pdf}\n",
       "%          \\caption{Weak scaling efficiency by model size. The batch size is fixed and model computational cost is scaled proportional to the number of GPUs.}\n",
       "%          \\label{fig:scaling_model}\n",
       "%      \\end{subfigure}\n",
       "%      \\quad\n",
       "%      \\begin{subfigure}[b]{0.29\\textwidth}\n",
       "%          \\centering\n",
       "%          \\includegraphics[scale=0.2]{gp_ddp_v1.png}\n",
       "%          \\caption{Graph (blue) and graph + data (red) parallel as a function of number of GPUs.}\n",
       "%          \\label{fig:gp_ddp}\n",
       "%      \\end{subfigure}\n",
       "% \\caption{(a,b) Weak scaling efficiency and (c) TeraFLOPs per second~\\textit{vs.} number of GPUs}\n",
       "%         % -- using graph parallel up to ${\\sim}50$ TFLOPs per second on 8 GPUs, and using\n",
       "%         % graph + data parallel up to ${\\sim}160$ TFLOPs per second on 256 GPUs.}\n",
       "%         \\vspace{-10pt}\n",
       "% \\end{figure}\n",
       "\n",
       "\\begin{figure}[h]\n",
       "     \\begin{subfigure}[b]{0.5\\textwidth}\n",
       "         \\centering\n",
       "         \\includegraphics[scale=0.3]{efficiency.pdf}\n",
       "         \\caption{Weak scaling efficiency.}\n",
       "        %  \\caption{Weak scaling efficiency measured by increasing the model size proportional to the number of GPUs.}\n",
       "         \\label{fig:weak_scaling}\n",
       "     \\end{subfigure}\n",
       "     \\quad\n",
       "     \\begin{subfigure}[b]{0.5\\textwidth}\n",
       "         \\centering\n",
       "         \\includegraphics[scale=0.3]{scaling.pdf}\n",
       "         \\caption{Comparison of graph and pipeline parallel training.}\n",
       "        %  \\caption{Comparing graph parallelism with pipeline parallelism. Solid blue, green and red curves show the scaling performance of graph, pipeline, and graph+pipeline parallel training. Dashed lines show the same with 32-way data parallel training. We do not show the results of training the largest models with pipeline parallelism as those runs ran out of GPU memory.}\n",
       "         \\label{fig:gp_ddp}\n",
       "     \\end{subfigure}\n",
       "\n",
       "\\caption{\\textbf{Left:} Weak scaling efficiency measured by scaling the model size proportional to the number of GPUs. \\textbf{Right:} Comparing graph parallelism with pipeline parallelism. Solid blue, green and red curves show the scaling performance of graph, pipeline, and graph+pipeline parallel training. Dashed lines show the same with 32-way data parallel training. We do not show the results of training the largest models with pipeline parallelism as those runs ran out of GPU memory.}\n",
       "% \\caption{(a,b) Weak scaling efficiency and (c) TeraFLOPs per second~\\textit{vs.} number of GPUs}\n",
       "        % -- using graph parallel up to ${\\sim}50$ TFLOPs per second on 8 GPUs, and using\n",
       "        % graph + data parallel up to ${\\sim}160$ TFLOPs per second on 256 GPUs.}\n",
       "        \\vspace{-10pt}\n",
       "\\end{figure}\n",
       "\n",
       "\\subsection{Scaling Analysis}\n",
       "\\label{sec:scaling}\n",
       "\n",
       "Weak scaling studies the effect on the throughput when computation is scaled proportional to the number of processors. We study weak scaling efficiency in terms of scaling the model size proportional to the number of GPUs. For these experiments, we use 4 different GemNet-T models, ranging from 120M parameters to nearly 1B parameters (see Table~\\ref{tab:scaling_models}).\n",
       "\n",
       "We train increasingly larger models on multiple GPUs that do not fit on a single GPU. Figure~\\ref{fig:weak_scaling} shows that we are able to obtain a scaling efficiency of roughly $79\\%$ with 8 GPUs for our largest model with nearly a billion parameters. This shows that our graph parallel training can be scaled up to billion parameter GemNet-T models while obtaining reasonably good scaling performance.\n",
       "\n",
       "Figure~\\ref{fig:weak_scaling} further shows the scaling efficiency for each model combining graph parallelism with 32-way data parallelism. Pure data parallel training with 32 GPUs obtains a scaling efficiency of $75\\%$ for the smallest model, showing the effect of network communication and load imbalance between the GPUs. Combining graph and data parallel training with 256 GPUs only reduces the scaling efficiency to $47\\%$ for the largest model compared to the 1 GPU case, suggesting the graph parallelism is promising for training extremely large models on hundreds of GPUs.\n",
       "\n",
       "Finally, figure~\\ref{fig:gp_ddp} shows the raw performance of running these models on V100 GPUs in terms of TeraFLOPs per second as a function of the number of GPUs. On a single GPU, the 120M parameter GemNet-T sustains 32 TeraFLOPs or roughly $25\\%$ of the theoretical peak FLOPS for a single GPU, and is thus a strong baseline. With 256 GPUs, the largest model sustains $3.5$ PetaFLOPs.\n",
       "\n",
       "Figure~\\ref{fig:gp_ddp} compares graph and pipeline parallelism~\\citep{huang2019gpipe} showing that graph parallelism outperforms pipeline parallelism for the models that we consider. Since each graph in the training data contains different numbers of nodes, edges and triplets, load balancing across GPUs is difficult for pipeline parallelism. Graph parallelism is able to overcome this problem since the nodes, edges and triplets of a given batch are always distributed evenly across the GPUs, helping it outperform pipeline parallelism.\n",
       "%\n",
       "It is possible, however, that pipeline parallelism might outperform graph parallelism for very deep GNNs since inter-GPU communication overhead for pipeline parallelism is independent of the number of blocks.\n",
       "Figure ~\\ref{fig:gp_ddp} also shows results with graph and pipeline parallelism combined,\n",
       "indicating that these methods are complementary to each other.\n",
       "%While training deeper GNNs, this allows one to make trade-offs between load balancing and number of inter-GPU communication steps.\n",
       "\n",
       "% \\textbf{Weak scaling}\n",
       "% % In this section, we analyze the scaling behavior of our models in terms of weak scaling efficiency.\n",
       "% studies the effect on the throughput when computation is scaled proportional to the number of processors.\n",
       "% We study weak scaling efficiency in terms of scaling batch size as well as scaling model size, both with graph parallelism.\n",
       "% For these experiments, we use 4 different GemNet-T models, ranging from 125.4 million parameters to 1.12 billion parameters (see Table~\\ref{tab:scaling_models}).\n",
       "\n",
       "% For weak scaling efficiency by batch size, we train the smallest GemNet-T model from Table~\\ref{tab:scaling_models}\n",
       "% and scale up batch size linearly with number of GPUs.\n",
       "% %\n",
       "% \\ad{Figure \\ref{fig:scaling_batch} shows that using graph parallelism to scale to 8x the batch size on 8 GPUs achieves ${\\sim}46\\%$ of linear scaling.\n",
       "%\n",
       "% For reference, if the throughput (time per batch) with 8x the batch size on 8 GPUs was the same as 1x the batch size on 1 GPU, efficiency would be $100\\%$.\n",
       "% %\n",
       "% In this case, since the model fits on a single GPU, one can alternatively use data parallelism and we expect it\n",
       "% to have better scaling efficiency with batch size because of significantly fewer quantities being communicated across GPUs than graph parallelism.}\n",
       "% we obtain roughly $46\\%$ scaling efficiency with 8 GPUs.\n",
       "\n",
       "% For weak scaling efficiency by model size, we train increasingly larger models on multiple GPUs\n",
       "% that do not fit on a single GPU.\n",
       "%\n",
       "% Figure~\\ref{fig:scaling_model} shows that we are able to obtain a scaling efficiency of roughly $50\\%$ with 8 GPUs for our largest model with 1.12B parameters.\n",
       "% %\n",
       "% \\ad{Here, data parallelism is not even applicable since this model does not fit on 1 GPU.\n",
       "% %\n",
       "% Further, we see that going from 500M parameters (on 4 GPUs) to 1.12B parameters (on 8 GPUs), scaling efficiency only decreases from\n",
       "% ${\\sim}54\\%$ to ${\\sim}50\\%$, suggesting that graph parallelism is promising in this large model size regime.}\n",
       "\n",
       "% \\ad{Finally, Figure~\\ref{fig:gp_ddp} shows that combining graph and data parallelism follows a similar linear\n",
       "% scaling behavior as graph parallel alone, suggesting that the two approaches are complementary.}\n",
       "\n",
       "\n",
       "\\section{Related Work}\n",
       "\\label{sec:related}\n",
       "\n",
       "\\paragraph{GNNs for simluating atomic systems}\n",
       "Many GNN based approaches have been proposed for the task of estimating atomic properties such as~\\citep{schutt2017quantum,gilmer2017neural,jorgensen2018neural,schutt2017schnet,schutt2018schnet,xie2018crystal,qiao2020orbnet,klicpera2020directional}, where the atoms are represented by nodes and neighboring atoms are connected by edges. An early approach for force estimation was the SchNet model \\cite{schutt2017schnet}, which computed forces using only the distance between atoms without the use of angular information. SchNet proposed the use of differentiable edge filters which enabled constructing energy-conserving models by estimating forces as the gradient of the energy.\n",
       "Subsequent work~\\citep{klicpera_dimenetpp_2020,klicpera2020directional,klicpera2021gemnet,liu2021spherical} has extended on the SchNet model by adding bond angles and dihedral angles, which has resulted in improved performance. These models make use of higher order interactions among nodes which make them highly compute and memory intensive. An alternate approach for estimating forces is to directly regress the forces as an output of the network. While this approach does not enforce energy conservation or rotational equivariance, recent work~\\citep{hu2021forcenet} has shown that such models can still accurately predict forces.\n",
       "\n",
       "\\textbf{Distributed GNN Training}.\n",
       "Research on distributed GNN training has focused on the regime of training small models on a single, large graph, for instance\n",
       "by sampling local neighborhoods around nodes to create mini-batches~\\citep{jangda2021accelerating,zhang2020agl,zhu2019aligraph}.\n",
       "%\n",
       "While these approaches can scale to very large graphs,\n",
       "they are not suitable for the task of modeling atomic systems where it is important to consider the entire graph for predicting the energy and forces.\n",
       "\n",
       "An alternate line of work, that is more similar to ours, keeps the entire graph in memory by efficiently partitioning the graph among multiple nodes \\citep{Jia2020ImprovingTA,Ma2019NeuGraphPD,tripathy2020reducing}. These methods still operate in a single graph regime that is partitioned ahead of time. %\\bw{ \"Thus, the focus of that work is on finding efficient partitions of the graph, which is not applicable to our problem since we operate on millions of graphs.\"}\n",
       "Thus the focus of that work is on finding efficient partitions of the graph, which is not applicable to our problem since we operate on millions of graphs.\n",
       "Further, these works do not train very large GNNs, or GNNs that operate on higher-order interactions (\\textit{e.g.} triplets), that are important for atomic systems.\n",
       "\n",
       "\n",
       "\\textbf{Model Parallelism}.\n",
       "methods focus on training large models that do not fit entirely on one GPU (even with a batch size of 1).\n",
       "%\n",
       "GPipe~\\citep{huang2019gpipe} splits different sequences of layers into different processors, and splits each training mini-batch into micro-batches to avoid idle time per GPU.\n",
       "%\n",
       "% GPipe~\\citep{huang2019gpipe} splits up deep networks depth-wise into several partitions of consecutive layers that are placed on different GPUs. During training, mini-batches are split up into micro-batches and pipelining is used to overlap communication with computation.\n",
       "Megatron-LM~\\citep{shoeybi2020megatronlm} splits the model breadth-wise, where Transformer layer weights are partitioned\n",
       "across multiple GPUs to distribute computation.\n",
       "%\n",
       "We see model and graph parallelism as complementary approaches that can be combined to train even larger models.\n",
       "% We leave this for future work.\n",
       "%\n",
       "\\section{Discussion}\n",
       "%\n",
       "We presented Graph Parallelism, a new method for training large GNNs for modeling atomic\n",
       "simulations, where modeling higher-order interactions between atoms (triplets / quadruplets) is critical.\n",
       "%\n",
       "We showed that training larger DimeNet++ and GemNet-T models can yield significant improvements on the OC20 dataset.\n",
       "%\n",
       "Although we demonstrated graph parallelism for just two GNNs, it is broadly applicable to a host of message-passing GNNs, including equivariant networks,\n",
       "that can be cast in the GN / EGN framework (Sec.~\\ref{sec:egn}) by appropriately picking update and aggregate functions.\n",
       "% %\n",
       "% To give a couple more examples, the model proposed in~\\cite{jing_iclr21} uses geometric representations for nodes and edges that are updated through message passing. The update functions involve the use of Geometric Vector Perceptrons (GVPs) that preserve equivariance. This model can be cast in the GN framework by using an update function that includes GVPs within it.\n",
       "% %\n",
       "% \\cite{schutt_icml21} present another equivariant GNN model called PAINN, that is composed of a series of Message and Update blocks. By viewing the computation within the message block as the EdgeUpdate function, the message passing step as the EdgeAggregate function and the Update block as the NodeUpdate function, we can see that this model can also be implemented as a GN.\n",
       "%\n",
       "\n",
       "Further, it is possible to combine graph parallelism with model parallel methods such as GPipe~\\citep{huang2019gpipe}\n",
       "to train even larger models, which could yield further improvements,\n",
       "as briefly explored in Sec~\\ref{sec:scaling}.\n",
       "%\n",
       "For force-centric GNNs,\n",
       "it should be possible to use graph parallel for `breadth-wise' scaling~\\textit{i.e.}, to split higher-order computations (\\textit{e.g.} triplets)\n",
       "across GPUs, and GPipe for `depth-wise' scaling~\\textit{i.e.}, to scale to larger number of message-passing blocks,\n",
       "sequentially split across GPUs.\n",
       "%\n",
       "For energy-centric GNNs \\textit{e.g.}, DimeNet++, this combination is less obvious since these models\n",
       "require an additional backward pass through the network to compute forces as gradients of\n",
       "energy with respect to atomic positions.\n",
       "%\n",
       "Energy-centric GNNs are common for atomic systems because they enforce the physical constraint of energy conservation.\n",
       "As we demonstrate with our DimeNet++-XL experiments, graph parallelism is applicable to energy-centric GNNs.\n",
       "\n",
       "We see scaling to large model sizes as a necessary (but not sufficient) step to effectively\n",
       "model atomic simulations from large, diverse datasets of adsorbates and catalysts.\n",
       "%\n",
       "Further progress may require marrying large scale with ways to better capture 3D geometry and physical priors.\n",
       "\n",
       "% paragraph on carbon footprint\n",
       "The carbon emissions from training large deep learning models are non-trivial and the work we have presented here is no exception~\\citep{strubell_arxiv19, schwartz_arxiv19}. We estimate that training our GemNet-XL model with Tesla v100 32 GB GPUs on cloud resources in the US ranges from 3490 - 8052 kg of CO$_2$ eq. ~\\citep{lacoste2019quantifying}. In the worst case, the emissions are roughly equivalent to 16 round trip flights from Los Angeles to New York. Assuming that the training time is fixed, emissions largely depend on the carbon intensity of the energy generation in a given region and the percentage of emissions offset with other investments by the resource provider. When choosing compute resources we recommend evaluating the stated carbon offset commitments and if possible, consider running experiments in regions that have more sustainable energy generation.\n",
       "The compute resources we utilized for this paper were committed to be $100\\%$ offset.\n",
       "\n",
       "\\bibliography{graphparallel}\n",
       "\\bibliographystyle{iclr2022_conference}\n",
       "\n",
       "\\clearpage\n",
       "\n",
       "\\appendix\n",
       "\\section{Appendix}\n",
       "% You may include other additional sections here.\n",
       "\\subsection{Additional results}\n",
       "\n",
       "Tables \\ref{tab:s2ef_results_full}, \\ref{tab:is2re_results_full} and \\ref{tab:is2rs_results_full} show results from each of the four test sets for the S2EF, IS2RE and IS2RS tasks respectively. DimeNet++-XL and GemNet-XL achieve the best results for each test set along each metric.\n",
       "\n",
       "\\begin{table*}[htb]\n",
       "    % \\small\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{5pt}\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{l|cccc}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mc{4}{c}{\\textbf{S2EF Test}}\\\\\n",
       "        & Energy MAE (EV)$\\downarrow$ & Force Cos$\\uparrow$ & Force MAE (EV/A)$\\downarrow$ & EFwT$\\uparrow$\\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{ID}} \\\\\n",
       "        GemNet-T &\t0.2257\t& 0.637\t& 0.02099\t& 2.4\\% \\\\\n",
       "        SpinConv & 0.2612 & 0.5479 & 0.02689 & 0.82\\% \\\\\n",
       "        ForceNet-Large & 2.0674\t& 0.533 & 0.02782\t& 0.02\\% \\\\\n",
       "        DimeNet++-large & 29.3333 & 0.5633 & 0.02807 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.2120} & \\textbf{0.6759} & \\textbf{0.0181} & \\textbf{3.30\\%} \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Ads}} \\\\\n",
       "        GemNet-T & 0.2099 &\t0.6242 & 0.02186 & 1.15\\% \\\\\n",
       "        SpinConv & 0.2752 & 0.5345 & 0.02769 & 0.38\\% \\\\\n",
       "        ForceNet-Large & 2.4188\t& 0.5212\t& 0.02834 & 0.01\\% \\\\\n",
       "        DimeNet++-large & 30.0338 & 0.5503 &\t0.02896 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.1980} & \\textbf{0.6642} & \\textbf{0.0186} & \\textbf{1.62\\%} \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Cat}} \\\\\n",
       "        GemNet-T & 0.3403 & 0.5813 & 0.02445 & 0.93\\% \\\\\n",
       "        SpinConv & 0.3501 & 0.5187 & 0.02849 & 0.46\\% \\\\\n",
       "        ForceNet-Large & 2.0203\t& 0.4936 & 0.03089 & 0.01\\% \\\\\n",
       "        DimeNet++-Large & 30.0437\t& 0.5109\t& 0.0312 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.3083} & \\textbf{0.6306} & \\textbf{0.0206} & \\textbf{1.72\\%} \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Both}} \\\\\n",
       "        GemNet-T & 0.3937 & 0.6221 & 0.02956 & 0.3\\% \\\\\n",
       "        SpinConv & 0.4585 & 0.5554 & 0.03556 & 0.14\\% \\\\\n",
       "        Forcenet-Large &\t2.5447\t& 0.5302 & 0.03754 & 0\\% \\\\\n",
       "        DimeNet++-Large & 36.7529 & 0.5517 & 0.03705 & 0\\% \\\\\n",
       "        GemNet-XL & \\textbf{0.362} & \\textbf{0.6704} & \\textbf{0.0245} & \\textbf{0.61\\%} \\\\\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Full set of results on the S2EF task comparing our GemNet-XL to the top three entries on the Open Catalyst leaderboard, showing metrics from each test set. }\n",
       "    \\label{tab:s2ef_results_full}\n",
       "\\end{table*}\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    % \\small\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{l|l|cc}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\mr{2}{\\textbf{Approach}} & \\mc{2}{c}{\\textbf{IS2RE Test}}\\\\\n",
       "        & & Energy MAE (EV)$\\downarrow$ & EwT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{ID}} \\\\\n",
       "        GemNet-T & Relaxation & \t0.3901\t& 12.37\\% \\\\\n",
       "        SpinConv & Relaxation & \t0.4207\t& 9.4\\%\t\\\\\n",
       "        Noisy Nodes & Direct & 0.4776\t& 5.71\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3764} & \\textbf{13.25\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4194 & 7.52\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{OOD Ads}} \\\\\n",
       "        GemNet-T & Relaxation &\t0.3907\t& 9.11\\% \\\\\n",
       "        SpinConv & Relaxation & \t0.4381\t& 7.47\\% \\\\\n",
       "        Noisy Nodes & Direct &\t0.5646\t& 3.49\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3677} & \\textbf{10.00\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.5258 & 3.95\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{OOD Cat}} \\\\\n",
       "        GemNet-T & Relaxation &\t0.4339 &\t10.09\\% \\\\\n",
       "        SpinConv & Relaxation & 0.4579\t& 8.16\\% \\\\\n",
       "        Noisy Nodes & Direct &\t0.4932\t& 5.02\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.4022} & \\textbf{11.61\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4373 & 6.76\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{4}{c}{\\textbf{OOD Both}} \\\\\n",
       "        GemNet-T & Relaxation &\t0.3843\t& 7.87\\% \\\\\n",
       "        SpinConv & Relaxation & 0.4203\t& 6.56\\% \\\\\n",
       "        Noisy Nodes & Direct & 0.5042\t& 3.82\\% \\\\\n",
       "        GemNet-XL & Relaxation & \\textbf{0.3383} & \\textbf{9.65\\%} \\\\\n",
       "        GemNet-XL-FT & Direct & 0.4665 & 4.19\\% \\\\\n",
       "\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Experimental results on the IS2RE task comparing our GemNet-XL to the top three entries on the Open Catalyst leaderboard, showing metrics from each test set. }\n",
       "    \\label{tab:is2re_results_full}\n",
       "\\end{table*}\n",
       "\n",
       "\n",
       "\\begin{table*}[t]\n",
       "    % \\small\n",
       "    \\centering\n",
       "    \\renewcommand{\\arraystretch}{1.0}\n",
       "    \\setlength{\\tabcolsep}{6pt}\n",
       "    % \\resizebox{0.97\\linewidth}{!}{\n",
       "    \\begin{tabular}{lrccc}\n",
       "        \\toprule\n",
       "        \\mr{2}{\\textbf{Model}} & \\textbf{Training} & \\mc{3}{c}{\\textbf{IS2RS Test}} \\\\\n",
       "        & \\textbf{Dataset} & AFbT$\\uparrow$ & ADwT$\\uparrow$ & FbT$\\uparrow$ \\\\\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{ID}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 33.75\\% & 59.18\\% & 2.0\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 25.65\\% & 52.45\\% & 1.0\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 21.10\\% &\t53.68\\% & 0.2\\%\t\\\\\n",
       "        DimeNet++ & S2EF 20M + MD & 21.08\\% & 48.6\\% & 0.2\\% \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD & \\textbf{40.00\\%} & 59.90\\% & \\textbf{2.4\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & 34.61 & \\textbf{62.73\\%} & \\textbf{2.4\\%} \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Ads}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 26.84\\% & 54.59\\% & 0.2\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 20.73\\% & 48.47\\% & 0.4\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 15.70\\% &\t48.87\\% & 0.0\\%\t\\\\\n",
       "        DimeNet++- & S2EF 20M + MD & 17.05\\% & 42.98\\% & 0.0\\% \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD & \\textbf{36.01\\%} & 55.68\\% & \\textbf{1.6\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & 30.32\\% & \\textbf{58.57\\%} & 0.6\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Cat}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 24.69\\% &\t58.71\\%\t& 0.4\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 20.24\\% & 50.99\\% & 0\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 15.86\\% &\t53.92\\% & 0\\%\t\\\\\n",
       "        DimeNet++- & S2EF 20M + MD & 16.43\\% & 48.19\\% & 0\\% \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD &  \\textbf{29.62\\%} & 58.43\\% & \\textbf{0.6\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & 29.33\\% & \\textbf{62.60\\%} & 0.2\\% \\\\\n",
       "\n",
       "        \\midrule\n",
       "        \\mc{5}{c}{\\textbf{OOD Both}} \\\\\n",
       "        GemNet-T & S2EF-ALL & 25.11\\% &\t62.23\\%\t& 0.2\\% \\\\\n",
       "        DimeNet++-large & S2EF-ALL & 20.67\\% & 54.82\\% & 0.2\\% \\\\\n",
       "        SpinConv & S2EF-ALL & 14.01\\% &\t58.03\\% & 0\\%\t\\\\\n",
       "        DimeNet++- & S2EF 20M + MD & 14.02\\% & 51.09\\% & \\textbf{0.4\\%} \\\\\n",
       "        DimeNet++-XL & S2EF 20M + MD & 28.14\\% & 62.85\\% & \\textbf{0.4\\%} \\\\\n",
       "        GemNet-XL & S2EF-ALL & \\textbf{29.02\\%} & \\textbf{66.72\\%} & \\textbf{0.4\\%} \\\\\n",
       "\n",
       "        \\bottomrule\n",
       "    \\end{tabular}\n",
       "    % }\n",
       "    \\caption{Experimental results on the IS2RS task comparing our models to the top four entries on the Open Catalyst leaderboard, showing metrics for each test dataset. The DimeNet++ and DimeNet++-XL models were trained on the S2EF 20M + MD dataset, that contains additional molecular dynamics data, which has been shown to be helpful for the IS2RS task.}\n",
       "    \\label{tab:is2rs_results_full}\n",
       "\\end{table*}\n",
       "\n",
       "\\end{document}\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = TexSoup(open(\"/workspaces/latexdl/data/2203.09697/2203.09697.tex\"))\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TexEnv' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mtype\u001b[39m(TexSoup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m This is a comment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpr)\n\u001b[1;32m     32\u001b[0m TexSoup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m This is a comment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mTexSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;132;43;01m{text}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m This is a comment\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m This is a comment\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m This is a comment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TexEnv' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from TexSoup.data import TexEnv, TexCmd, TexText, TexGroup\n",
    "\n",
    "\n",
    "def to_dictionary(tex_tree):\n",
    "    str_tree = []\n",
    "    for i in tex_tree:\n",
    "        if isinstance(i, list):\n",
    "            str_tree.append(i)\n",
    "        elif isinstance(i, TexEnv):\n",
    "            str_tree.append(\n",
    "                {\n",
    "                    i.name: [\n",
    "                        {\"begin\": i.begin + str(i.args)},\n",
    "                        to_dictionary(i.all),\n",
    "                        {\"end\": i.end},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        elif isinstance(i, TexCmd):\n",
    "            str_tree.append({i.name: \"\\\\\" + i.name + str(i.args)})\n",
    "        elif isinstance(i, TexText):\n",
    "            str_tree.append(str(i.text))\n",
    "        elif isinstance(i, TexGroup):\n",
    "            str_tree.append([\"{\", to_dictionary(TexSoup(i.value).expr.all), \"}\"])\n",
    "        else:\n",
    "            str_tree.append(str(i))\n",
    "\n",
    "    return str_tree\n",
    "\n",
    "\n",
    "type(TexSoup(\"% This is a comment\").expr)\n",
    "TexSoup(\"% This is a comment\")\n",
    "TexSoup(\n",
    "    [\"\\\\input{text}\\n% This is a comment\\n% This is a comment\\n% This is a comment\"]\n",
    ").expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TexCmd('caption', [BraceGroup(' 30 ', '\\\\%')])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TexSoup(r\"\"\"\\caption{ 30 \\%}\"\"\").expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TexSoup.data.TexEnv'> [TexCmd('xd', [BraceGroup(TexCmd('caption', [BraceGroup('  ', TexCmd('capteions', [BraceGroup()]), ' ', TexCmd('capteiongsd', [BracketGroup('4321'), BraceGroup('30')]), ' ')]))]), '% hello', '\\nterst\\ntes', '%'] \\xd{\\caption{  \\capteions{} \\capteiongsd[4321]{30} }}% hello\n",
      "terst\n",
      "tes% [tex]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'expr'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'xd'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'caption'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'  '</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'capteions'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">()])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' '</span>,\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'capteiongsd'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BracketGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'4321'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'30'</span><span style=\"font-weight: bold\">)])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' '</span><span style=\"font-weight: bold\">)]))])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'% hello'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'\\nterst\\ntes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'%'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'parent'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'char_to_line'</span>: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">TexSoup.utils.CharToLineOffset</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7014666d3fd0</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'expr'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'xd'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'caption'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'  '\u001b[0m, \u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'capteions'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m' '\u001b[0m,\n",
       "\u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'capteiongsd'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBracketGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'4321'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'30'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m' '\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'% hello'\u001b[0m, \u001b[32m'\\nterst\\ntes'\u001b[0m, \u001b[32m'%'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'char_to_line'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;95mTexSoup.utils.CharToLineOffset\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7014666d3fd0\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'[tex]'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'parent'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'_contents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'xd'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'caption'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'  '</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'capteions'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">()])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' '</span>, \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TexCmd</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'capteiongsd'</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BracketGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'4321'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BraceGroup</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'30'</span><span style=\"font-weight: bold\">)])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' '</span><span style=\"font-weight: bold\">)]))])</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'% hello'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'\\nterst\\ntes'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'%'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'preserve_whitespace'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'position'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'_begin'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'_end'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'name'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtex\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'args'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'_contents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'xd'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'caption'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'  '\u001b[0m, \u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'capteions'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m' '\u001b[0m, \n",
       "\u001b[1;35mTexCmd\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'capteiongsd'\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mBracketGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'4321'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mBraceGroup\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'30'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m' '\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'% hello'\u001b[0m,\n",
       "        \u001b[32m'\\nterst\\ntes'\u001b[0m,\n",
       "        \u001b[32m'%'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'preserve_whitespace'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'position'\u001b[0m: \u001b[1;36m-1\u001b[0m,\n",
       "    \u001b[32m'_begin'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "    \u001b[32m'_end'\u001b[0m: \u001b[32m''\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rich\n",
    "\n",
    "ts = TexSoup(\n",
    "    \"\"\"\\\\xd{\\\\caption{  \\\\capteions{} \\\\capteiongsd[4321]{30} }}% hello\\nterst\\ntes%\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    type(ts.expr),\n",
    "    repr(ts.expr),\n",
    "    str(ts.expr),\n",
    "    ts.name,\n",
    ")\n",
    "\n",
    "\n",
    "rich.print(ts.__dict__)\n",
    "rich.print(ts.expr.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\\capteions<span style=\"font-weight: bold\">{}</span>, \\capteiongsd<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4321</span><span style=\"font-weight: bold\">]{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"font-weight: bold\">}]</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\\capteions\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \\capteiongsd\u001b[1m[\u001b[0m\u001b[1;36m4321\u001b[0m\u001b[1m]\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(\n",
    "    ts.contents[0].contents[0].contents, len(ts.contents[0].contents[0].contents)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '%', 'position': 64, 'category': <TokenCode.Comment: 25>}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.expr.all[-1].contents[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4321', '30']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.all[0].all[0].all[1].contents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latexdl-v9iRyZz--py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
